# pip install requests pandas python-dateutil

import os
import time
import logging
import requests
import pandas as pd
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Optional, Tuple, Set
from dataclasses import dataclass
from requests.adapters import HTTPAdapter, Retry

# -------------------- ë¡œê¹… --------------------
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('belle0_analysis.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

UTC = timezone.utc
KST = timezone(timedelta(hours=9))

# -------------------- ì„¤ì • --------------------
@dataclass
class Config:
    MIN_CLUSTER_SIZE: int = 3
    CLUSTER_WINDOW_MINUTES: int = 120
    HIGH_FAIL_RATE_THRESHOLD: float = 0.8
    CONSECUTIVE_FAIL_THRESHOLD: int = 10
    MAX_RETRIES: int = 3
    BATCH_SIZE: int = 1000          # txlist offset (<=1000 ê¶Œì¥)
    REQUEST_DELAY: float = 0.30     # 5 rps ì—¬ìœ 
    REQUEST_TIMEOUT: int = 30
    HOLDER_LIMIT: int = 557         # í™€ë” ê°œìˆ˜ë¥¼ 557ê°œë¡œ ë³€ê²½
    USE_RECEIPT_GAS: bool = False   # Trueë©´ ì¼ë¶€ Tx receiptë¡œ ë³´ì •
    ENRICH_RECEIPT_TOPN: int = 200  # receipt ë³´ì • ì‹¤íŒ¨ Tx ìµœëŒ€ ìˆ˜

# -------------------- ìœ í‹¸ --------------------
def parse_window_from_env() -> Tuple[Optional[datetime], Optional[datetime], str]:
    """
    ì‹œê°„ ìœˆë„ìš°ë¥¼ í™˜ê²½ë³€ìˆ˜ì—ì„œ ì½ì–´ UTCë¡œ ë°˜í™˜.
    ìš°ì„ ìˆœìœ„:
      1) WINDOW_START_KST, WINDOW_END_KST  (KST ë¡œ í•´ì„)
      2) WINDOW_START_UTC, WINDOW_END_UTC  (UTC ë¡œ í•´ì„)
      3) ì—†ìŒ -> (None, None)
    í¬ë§·: YYYY-MM-DD HH:MM:SS
    """
    fmt = "%Y-%m-%d %H:%M:%S"
    ks = os.getenv("WINDOW_START_KST", "").strip()
    ke = os.getenv("WINDOW_END_KST", "").strip()
    us = os.getenv("WINDOW_START_UTC", "").strip()
    ue = os.getenv("WINDOW_END_UTC", "").strip()

    if ks and ke:
        try:
            start_kst = datetime.strptime(ks, fmt).replace(tzinfo=KST)
            end_kst   = datetime.strptime(ke, fmt).replace(tzinfo=KST)
            return start_kst.astimezone(UTC), end_kst.astimezone(UTC), "KST"
        except Exception as e:
            logger.warning(f"Failed to parse KST window: {e}")

    if us and ue:
        try:
            start_utc = datetime.strptime(us, fmt).replace(tzinfo=UTC)
            end_utc   = datetime.strptime(ue, fmt).replace(tzinfo=UTC)
            return start_utc, end_utc, "UTC"
        except Exception as e:
            logger.warning(f"Failed to parse UTC window: {e}")

    return None, None, ""

def within_window(ts_utc: datetime, start_utc: Optional[datetime], end_utc: Optional[datetime]) -> bool:
    if start_utc and ts_utc < start_utc:
        return False
    if end_utc and ts_utc > end_utc:
        return False
    return True

# -------------------- ë©”ì¸ í´ë˜ìŠ¤ --------------------
class EtherscanAnalyzer:
    def __init__(self, api_key: str, config: Config = None):
        self.api_key = api_key or os.getenv("ETHERSCAN_API_KEY")
        if not self.api_key or self.api_key == "YOUR_API_KEY":
            raise ValueError("ETHERSCAN_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        self.config = config or Config()
        self.base_url = "https://api.etherscan.io/api"         # v1
        self.base_url_v2 = "https://api.etherscan.io/v2/api"   # v2 (beta)

        # ì„¸ì…˜ + Retry
        self.session = requests.Session()
        retries = Retry(
            total=5, backoff_factor=0.6,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["GET"]
        )
        self.session.mount("https://", HTTPAdapter(max_retries=retries))

        # ì‹œê°„ ìœˆë„ìš°(UTC) ì„¤ì •
        self.window_start_utc, self.window_end_utc, self.window_kind = parse_window_from_env()
        if self.window_start_utc or self.window_end_utc:
            s = self.window_start_utc.astimezone(KST).strftime("%Y-%m-%d %H:%M:%S %Z") if self.window_start_utc else "-"
            e = self.window_end_utc.astimezone(KST).strftime("%Y-%m-%d %H:%M:%S %Z") if self.window_end_utc else "-"
            logger.info(f"Applied time window [{self.window_kind}]: {s} ~ {e}")

        # ë¼ìš°í„° ì£¼ì†Œë“¤(í•„ìš”ì‹œ ì»¤ìŠ¤í…€ ë¼ìš°í„° ìˆ˜ë™ ì¶”ê°€)
        self.routers: Set[str] = {
            "0x7a250d5630b4cf539739df2c5dacb4c659f2488d",  # Uniswap V2
            "0xe592427a0aece92de3edee1f18e0157c05861564",  # Uniswap V3
            "0x68b3465833fb72a70ecdf485e0e4c7bd8665fc45",  # Uniswap V3 Router 2
            "0x1111111254fb6c44bac0bed2854e76f90643097d",  # 1inch v4
            "0x1111111254eeb25477b68fb85ed929f73a960582",  # 1inch v5
            "0xdef1c0ded9bec7f1a1670819833240f027b25eff",  # 0x
            "0x881d40237659c251811cec9c364ef91dc08d300c",  # Metamask
        }
        # ì˜ˆ) ì˜ì‹¬ ë¼ìš°í„°ê°€ ìˆë‹¤ë©´ ì—¬ê¸°ì— ì§ì ‘ ì¶”ê°€
        suspect_router = os.getenv("EXTRA_ROUTER", "").strip().lower()
        if suspect_router.startswith("0x") and len(suspect_router) == 42:
            self.routers.add(suspect_router)
            logger.info(f"Added EXTRA_ROUTER: {suspect_router}")

        # í•¨ìˆ˜ ì‹œê·¸ë‹ˆì²˜ ë§¤í•‘
        self.sig_map: Dict[str, str] = {
            # Uniswap V2
            "0x18cbafe5": "swapExactTokensForETH",                                   # SELL
            "0x791ac947": "swapExactTokensForETHSupportingFeeOnTransferTokens",      # SELL
            "0x4a25d94a": "swapTokensForExactETH",                                   # SELL
            "0x7ff36ab5": "swapExactETHForTokens",                                   # BUY
            "0xfb3bdb41": "swapETHForExactTokens",                                   # BUY
            "0x38ed1739": "swapExactTokensForTokens",
            "0x8803dbee": "swapTokensForExactTokens",
            "0xb6f9de95": "swapExactETHForTokensSupportingFeeOnTransferTokens",      # BUY
            # 1inch
            "0x12aa3caf": "swap",        # v5
            "0x0502b1c5": "unoswap",
            "0x2e95b6c8": "unoswap",
        }

    # -------------------- í™€ë” ìˆ˜ì§‘ --------------------
    def get_top_holders_v2(self, token_addr: str, limit: int) -> List[str]:
        params = {
            "chainid": 1,
            "module": "token",
            "action": "topholders",
            "contractaddress": token_addr,
            "offset": min(limit, 1000),
            "apikey": self.api_key,
        }
        r = self.session.get(self.base_url_v2, params=params, timeout=self.config.REQUEST_TIMEOUT)
        r.raise_for_status()
        data = r.json()
        result = data.get("result", {})
        holders = result.get("holders", [])
        if isinstance(holders, list) and holders:
            return [h.get("address", "").lower() for h in holders if h.get("address")][:limit]
        if isinstance(result, list) and result:
            return [h.get("address", "").lower() for h in result if h.get("address")][:limit]
        return []

    def get_holders_from_tokentx_v1(self, token_addr: str, limit: int) -> List[str]:
        page = 1
        offset = min(10000, self.config.BATCH_SIZE)
        max_pages = min(100, 10000 // max(1, offset))
        addrs: Set[str] = set()
        zero = "0x0000000000000000000000000000000000000000"
        token_addr_l = token_addr.lower()

        while page <= max_pages:
            params = {
                "module": "account",
                "action": "tokentx",
                "contractaddress": token_addr,
                "page": page,
                "offset": offset,
                "sort": "asc",
                "apikey": self.api_key
            }
            r = self.session.get(self.base_url, params=params, timeout=self.config.REQUEST_TIMEOUT)
            r.raise_for_status()
            data = r.json()
            if data.get("status") == "0" and "No transactions found" in data.get("message", ""):
                break
            rows = data.get("result", []) or []
            if not rows:
                break

            for t in rows:
                frm = (t.get("from") or "").lower()
                to = (t.get("to") or "").lower()
                if frm and frm != zero and frm != token_addr_l:
                    addrs.add(frm)
                if to and to != zero and to != token_addr_l:
                    addrs.add(to)

            if len(rows) < offset:
                break
            page += 1
            time.sleep(self.config.REQUEST_DELAY)
            if len(addrs) >= limit:
                break

        res = list(addrs)[:limit]
        logger.info(f"Collected {len(res)} holder candidates from tokentx (v1)")
        return res

    def get_holders(self, token_addr: str, limit: int) -> List[str]:
        try:
            addrs = self.get_top_holders_v2(token_addr, limit)
            if addrs:
                logger.info(f"Fetched {len(addrs)} top holders via v2")
                return addrs
            logger.warning("v2 topholders returned empty, falling back to v1 tokentx")
        except Exception as e:
            logger.warning(f"v2 topholders failed ({e}), falling back to v1 tokentx")
        return self.get_holders_from_tokentx_v1(token_addr, limit)

    # -------------------- íŠ¸ëœì­ì…˜ ìˆ˜ì§‘/ì •ê·œí™” --------------------
    def _validate_api_response(self, data: Dict) -> bool:
        if not isinstance(data, dict):
            return False
        if data.get("message") == "NOTOK":
            result = str(data.get("result", ""))
            if "rate limit" in result.lower():
                logger.warning("Rate limit hit, waiting...")
                time.sleep(1)
                return False
            logger.warning(f"API returned NOTOK: {result}")
            return False
        return True

    def get_txlist(self, address: str, startblock: int = 0, endblock: int = 99999999, sort: str = "asc") -> List[Dict]:
        logger.info(f"Fetching transactions for address: {address}")
        page, offset = 1, self.config.BATCH_SIZE
        max_pages = min(100, 10000 // max(1, offset))
        all_rows: List[Dict] = []

        while True:
            if page > max_pages:
                logger.warning(f"Reached Etherscan window limit ({max_pages} pages) for {address}")
                break

            params = {
                "module": "account",
                "action": "txlist",
                "address": address,
                "startblock": startblock,
                "endblock": endblock,
                "page": page,
                "offset": offset,
                "sort": sort,
                "apikey": self.api_key
            }

            for attempt in range(self.config.MAX_RETRIES):
                try:
                    response = self.session.get(self.base_url, params=params, timeout=self.config.REQUEST_TIMEOUT)
                    response.raise_for_status()
                    data = response.json()
                    if not self._validate_api_response(data):
                        if attempt == self.config.MAX_RETRIES - 1:
                            logger.error(f"Failed after {self.config.MAX_RETRIES} attempts for {address}")
                            return all_rows
                        time.sleep(2 ** attempt)
                        continue

                    if data.get("status") == "0" and "No transactions found" in data.get("message", ""):
                        logger.info(f"No transactions found for {address}")
                        return all_rows

                    batch = data.get("result", []) or []
                    if not isinstance(batch, list):
                        return all_rows
                    if not batch:
                        return all_rows

                    all_rows.extend(batch)
                    logger.info(f"Fetched {len(batch)} transactions (page {page}) for {address}")

                    if len(batch) < offset:
                        return all_rows

                    page += 1
                    time.sleep(max(self.config.REQUEST_DELAY, 0.25))
                    break

                except requests.RequestException as e:
                    logger.warning(f"Request attempt {attempt + 1} failed for {address}: {e}")
                    if attempt == self.config.MAX_RETRIES - 1:
                        logger.error(f"All retry attempts failed for {address}")
                        return all_rows
                    time.sleep(2 ** attempt)
            else:
                break

        logger.info(f"Total {len(all_rows)} transactions fetched for {address}")
        return all_rows

    def _is_sell(self, method_name: str) -> bool:
        if not method_name:
            return False
        name = method_name.lower()
        return ("tokensforeth" in name)  # swapExactTokensForETH / swapTokensForExactETH ë“±

    def normalize_tx_rows(self, rows: List[Dict], holder: str) -> pd.DataFrame:
        if not rows:
            return pd.DataFrame()

        processed_rows = []
        for tx in rows:
            try:
                if not all(k in tx for k in ["hash", "timeStamp", "blockNumber", "to"]):
                    continue

                to_addr = (tx.get("to") or "").lower()
                if to_addr not in self.routers:
                    continue  # ë¼ìš°í„° í˜¸ì¶œë§Œ

                # ì‹œê°„ íŒŒì‹± (UTC aware) + ìœˆë„ìš° í•„í„°
                ts = int(tx["timeStamp"])
                time_utc = datetime.fromtimestamp(ts, UTC)
                if not within_window(time_utc, self.window_start_utc, self.window_end_utc):
                    continue  # <<<<<<<<<<<< ì‹œê°„ ìœˆë„ìš° ì ìš© í•µì‹¬

                input_data = tx.get("input", "") or ""
                method_sig = input_data[:10].lower() if (input_data.startswith("0x") and len(input_data) >= 10) else ""
                method_name = self.sig_map.get(method_sig, f"sig:{method_sig}" if method_sig else "unknown")

                is_error = str(tx.get("isError", "0")) == "1"
                receipt_failed = str(tx.get("txreceipt_status", "1")) == "0"
                status = "fail" if (is_error or receipt_failed) else "success"

                block_num = int(tx["blockNumber"])
                gas_used = int(tx.get("gasUsed", 0))
                gas_price = int(tx.get("gasPrice", 0))
                value_eth = float(tx.get("value", 0)) / 1e18
                gas_fee_eth = (gas_used * gas_price) / 1e18

                processed_rows.append({
                    "holder": holder.lower(),
                    "hash": tx["hash"],
                    "time_utc": time_utc,
                    "block": block_num,
                    "to": to_addr,
                    "method_guess": method_name,
                    "is_sell_path": self._is_sell(method_name),
                    "value_eth": value_eth,
                    "gas_used": gas_used,
                    "gas_price_gwei": gas_price / 1e9 if gas_price else 0.0,
                    "gas_fee_eth": gas_fee_eth,
                    "isError": tx.get("isError", "0"),
                    "txreceipt_status": tx.get("txreceipt_status", "1"),
                    "status": status,
                })
            except Exception as e:
                logger.warning(f"Error processing tx {tx.get('hash', '')}: {e}")
                continue

        df = pd.DataFrame(processed_rows)
        if not df.empty:
            df = df.drop_duplicates("hash", keep="first")
        logger.info(f"Processed {len(df)} router transactions in window for {holder}")
        return df

    # -------------------- ë¶„ì„ --------------------
    def _enrich_gas_with_receipts(self, df: pd.DataFrame) -> None:
        if df.empty:
            return
        target = df[df["status"] == "fail"].sort_values("time_utc", ascending=False).head(self.config.ENRICH_RECEIPT_TOPN)
        for h in target["hash"].tolist():
            try:
                r = self.session.get(self.base_url, params={
                    "module": "proxy",
                    "action": "eth_getTransactionReceipt",
                    "txhash": h,
                    "apikey": self.api_key
                }, timeout=self.config.REQUEST_TIMEOUT)
                js = r.json()
                result = js.get("result") or {}
                eff = result.get("effectiveGasPrice")
                gas_used_hex = result.get("gasUsed")
                if eff and gas_used_hex:
                    eff_int = int(eff, 16)
                    used = int(gas_used_hex, 16)
                    df.loc[df["hash"] == h, "gas_price_gwei"] = eff_int / 1e9
                    df.loc[df["hash"] == h, "gas_fee_eth"] = (eff_int * used) / 1e18
            except Exception:
                pass

    def cluster_failures(self, df: pd.DataFrame, window_minutes: Optional[int] = None) -> pd.DataFrame:
        if df.empty:
            return pd.DataFrame(columns=["holder", "start", "end", "count", "gas_waste_eth"])
        window_minutes = window_minutes or self.config.CLUSTER_WINDOW_MINUTES
        fails = df[df["status"] == "fail"].sort_values("time_utc").copy()
        if fails.empty:
            return pd.DataFrame(columns=["holder", "start", "end", "count", "gas_waste_eth"])

        clusters = []
        i = 0
        while i < len(fails):
            start_time = fails.iloc[i]["time_utc"]
            start_idx = i
            while i + 1 < len(fails):
                next_time = fails.iloc[i + 1]["time_utc"]
                if (next_time - start_time).total_seconds() <= window_minutes * 60:
                    i += 1
                else:
                    break
            end_idx = i
            count = end_idx - start_idx + 1
            if count >= self.config.MIN_CLUSTER_SIZE:
                cluster_txs = fails.iloc[start_idx:end_idx + 1]
                gas_waste = cluster_txs["gas_fee_eth"].sum()
                clusters.append({
                    "holder": fails.iloc[start_idx]["holder"],
                    "start": start_time,
                    "end": fails.iloc[end_idx]["time_utc"],
                    "count": count,
                    "gas_waste_eth": gas_waste
                })
            i += 1
        return pd.DataFrame(clusters)

    def detect_honeypot_indicators(self, summary_df: pd.DataFrame) -> Dict[str, List[str]]:
        indicators = {}
        for _, row in summary_df.iterrows():
            holder = row['holder']
            holder_indicators = []
            if row['fail_rate'] >= self.config.HIGH_FAIL_RATE_THRESHOLD:
                holder_indicators.append(f"ë†’ì€ ì‹¤íŒ¨ìœ¨: {row['fail_rate']:.1%}")
            if row['max_consecutive_fails'] >= self.config.CONSECUTIVE_FAIL_THRESHOLD:
                holder_indicators.append(f"ì—°ì† ì‹¤íŒ¨: {row['max_consecutive_fails']}íšŒ")
            if row['clusters_2h_count'] >= 3:
                holder_indicators.append(f"ì‹¤íŒ¨ í´ëŸ¬ìŠ¤í„°: {row['clusters_2h_count']}ê°œ")
            if row['total_gas_waste_eth'] > 0.1:
                holder_indicators.append(f"ê°€ìŠ¤ ë‚­ë¹„: {row['total_gas_waste_eth']:.3f} ETH")
            if 'sell_fail_rate' in row and row['sell_txs'] >= 5 and row['sell_fail_rate'] >= 0.5:
                holder_indicators.append(f"ë§¤ë„ ì‹¤íŒ¨ìœ¨ ë†’ìŒ(ì „ìš©): {row['sell_fail_rate']:.1%} / {row['sell_txs']}ê±´")
            if holder_indicators:
                indicators[holder] = holder_indicators
        return indicators

    def analyze_holder(self, address: str) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        logger.info(f"Starting analysis for holder: {address}")
        raw_txs = self.get_txlist(address)
        df = self.normalize_tx_rows(raw_txs, address)

        if df.empty:
            empty_summary = pd.DataFrame([{
                "holder": address.lower(),
                "tx_to_router": 0,
                "fails": 0,
                "success": 0,
                "fail_rate": 0.0,
                "max_consecutive_fails": 0,
                "avg_gas_failed": 0.0,
                "total_gas_waste_eth": 0.0,
                "clusters_2h_count": 0,
                "sell_txs": 0,
                "sell_fail_rate": 0.0,
                "first_tx": None,
                "last_tx": None
            }])
            return df, empty_summary, pd.DataFrame()

        if self.config.USE_RECEIPT_GAS:
            self._enrich_gas_with_receipts(df)

        df = df.sort_values("time_utc").reset_index(drop=True)

        total_txs = len(df)
        failed_txs = int((df["status"] == "fail").sum())
        success_txs = total_txs - failed_txs
        fail_rate = failed_txs / total_txs if total_txs else 0.0

        # ì—°ì† ì‹¤íŒ¨
        consecutive_fails = 0
        max_consecutive_fails = 0
        for status in df["status"]:
            if status == "fail":
                consecutive_fails += 1
                max_consecutive_fails = max(max_consecutive_fails, consecutive_fails)
            else:
                consecutive_fails = 0

        # ê°€ìŠ¤
        failed_df = df[df["status"] == "fail"]
        avg_gas_failed = float(failed_df["gas_used"].mean()) if not failed_df.empty else 0.0
        total_gas_waste = float(failed_df["gas_fee_eth"].sum()) if not failed_df.empty else 0.0

        # ë§¤ë„ ê²½ë¡œ ì „ìš© (ë³´ì¡°)
        sell_df = df[df["is_sell_path"] == True]
        sell_fail_rate = float((sell_df["status"] == "fail").mean()) if not sell_df.empty else 0.0

        # í´ëŸ¬ìŠ¤í„°
        clusters = self.cluster_failures(df)

        summary = pd.DataFrame([{
            "holder": address.lower(),
            "tx_to_router": total_txs,
            "fails": failed_txs,
            "success": success_txs,
            "fail_rate": round(fail_rate, 4),
            "max_consecutive_fails": int(max_consecutive_fails),
            "avg_gas_failed": round(avg_gas_failed, 0),
            "total_gas_waste_eth": round(total_gas_waste, 6),
            "clusters_2h_count": len(clusters),
            "sell_txs": int(len(sell_df)),
            "sell_fail_rate": round(sell_fail_rate, 4),
            "first_tx": df["time_utc"].min(),
            "last_tx": df["time_utc"].max()
        }])

        logger.info(f"Analysis complete for {address}: {total_txs} txs (window), {fail_rate:.1%} fail rate")
        return df, summary, clusters

    def safe_concat_dataframes(self, df_list: List[pd.DataFrame], name: str) -> pd.DataFrame:
        if not df_list:
            logger.warning(f"No dataframes to concat for {name}")
            return pd.DataFrame()
        try:
            result = pd.concat(df_list, ignore_index=True)
            logger.info(f"Successfully concatenated {len(df_list)} dataframes for {name}")
            return result
        except Exception as e:
            logger.error(f"Error concatenating dataframes for {name}: {e}")
            return pd.DataFrame()

    def analyze_multiple_holders(self, holders: List[str]) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        logger.info(f"Starting analysis for {len(holders)} holders")
        all_transactions, all_summaries, all_clusters = [], [], []

        for i, holder in enumerate(holders, 1):
            logger.info(f"Processing holder {i}/{len(holders)}: {holder}")
            try:
                tx_df, summary_df, cluster_df = self.analyze_holder(holder)
                if not tx_df.empty:
                    all_transactions.append(tx_df)
                if not summary_df.empty:
                    all_summaries.append(summary_df)
                if not cluster_df.empty:
                    all_clusters.append(cluster_df)
            except Exception as e:
                logger.error(f"Error analyzing holder {holder}: {e}")
                continue

        combined_txs = self.safe_concat_dataframes(all_transactions, "transactions")
        combined_summaries = self.safe_concat_dataframes(all_summaries, "summaries")
        combined_clusters = self.safe_concat_dataframes(all_clusters, "clusters")
        return combined_txs, combined_summaries, combined_clusters

    # -------------------- ì¶œë ¥/ì €ì¥ --------------------
    def generate_report(self, tx_df: pd.DataFrame, summary_df: pd.DataFrame, cluster_df: pd.DataFrame) -> str:
        report = []
        report.append("=" * 80)
        report.append("BELLE í† í° í—ˆë‹ˆíŒŸ ë¶„ì„ ë³´ê³ ì„œ (557ê°œ í™€ë”)")
        report.append("=" * 80)

        # í˜„ì¬ ì‹œê° ë° ìœˆë„ìš° í‘œê¸°
        report.append(f"ë¶„ì„ ì‹œê°„: {datetime.now(UTC).astimezone(KST).strftime('%Y-%m-%d %H:%M:%S %Z')}")
        if self.window_start_utc or self.window_end_utc:
            s = self.window_start_utc.astimezone(KST).strftime('%Y-%m-%d %H:%M:%S %Z') if self.window_start_utc else "-"
            e = self.window_end_utc.astimezone(KST).strftime('%Y-%m-%d %H:%M:%S %Z') if self.window_end_utc else "-"
            report.append(f"ì ìš© ìœˆë„ìš°: {s} ~ {e}")
        report.append("")

        if summary_df.empty:
            report.append("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return "\n".join(report)

        total_holders = len(summary_df)
        total_txs = int(summary_df["tx_to_router"].sum())
        total_fails = int(summary_df["fails"].sum())
        overall_fail_rate = (total_fails / total_txs) if total_txs > 0 else 0.0
        total_gas_waste = float(summary_df["total_gas_waste_eth"].sum())

        report.append("ğŸ“Š ì „ì²´ í†µê³„:")
        report.append(f"  - ë¶„ì„ í™€ë” ìˆ˜: {total_holders} / 557 (ëª©í‘œ)")
        report.append(f"  - ì´ ë¼ìš°í„° ê±°ë˜(ìœˆë„ìš° ë‚´): {total_txs:,}")
        report.append(f"  - ì‹¤íŒ¨ ê±°ë˜: {total_fails:,}")
        report.append(f"  - ì „ì²´ ì‹¤íŒ¨ìœ¨: {overall_fail_rate:.1%}")
        report.append(f"  - ì´ ê°€ìŠ¤ ë‚­ë¹„: {total_gas_waste:.4f} ETH")
        report.append("")

        indicators = self.detect_honeypot_indicators(summary_df)
        if indicators:
            report.append("ğŸš¨ í—ˆë‹ˆíŒŸ ì§€í‘œ íƒì§€:")
            for holder, holder_indicators in indicators.items():
                report.append(f"  {holder[:10]}...:")
                for indicator in holder_indicators:
                    report.append(f"    âš ï¸  {indicator}")
            report.append("")

        report.append("ğŸ‘¥ í™€ë”ë³„ ë¶„ì„ ê²°ê³¼:")
        for _, row in summary_df.iterrows():
            report.append(f"  {row['holder'][:10]}...:")
            report.append(
                f"    ê±°ë˜: {row['tx_to_router']:,} | ì‹¤íŒ¨ìœ¨: {row['fail_rate']:.1%} | "
                f"ì—°ì†ì‹¤íŒ¨: {row['max_consecutive_fails']} | ë§¤ë„ì‹¤íŒ¨ìœ¨(ì „ìš©): {row.get('sell_fail_rate',0):.1%} / {row.get('sell_txs',0)}ê±´"
            )
            report.append(f"    ê°€ìŠ¤ë‚­ë¹„: {row['total_gas_waste_eth']:.4f} ETH | í´ëŸ¬ìŠ¤í„°: {row['clusters_2h_count']}")
        report.append("")

        # ê²°ë¡ (ìœˆë„ìš° í•œì •)
        high_risk_holders = len([h for h, inds in indicators.items() if len(inds) >= 2])
        report.append("ğŸ“‹ ë¶„ì„ ê²°ë¡ (ìœˆë„ìš° ì ìš©):")
        if high_risk_holders > 0:
            report.append(f"  ğŸ”´ HIGH RISK: {high_risk_holders}/{total_holders} í™€ë”ê°€ í—ˆë‹ˆíŒŸ ì§€í‘œ ë‹¤ìˆ˜ ë³´ìœ ")
            report.append("      â†’ ë§¤ë„ ì°¨ë‹¨ í—ˆë‹ˆíŒŸ ìŠ¤ìº  í† í°ìœ¼ë¡œ íŒë‹¨ë¨")
        else:
            report.append("  ğŸŸ¡ ì¼ë¶€ ì§€í‘œ ì¡´ì¬í•˜ë‚˜ ê²°ì •ì  ì¦ê±° ë¶€ì¡±")

        report.append("=" * 80)
        return "\n".join(report)

    def save_results(self, tx_df: pd.DataFrame, summary_df: pd.DataFrame, cluster_df: pd.DataFrame) -> Dict[str, str]:
        timestamp = datetime.now(UTC).strftime("%Y%m%d_%H%M%S")
        filenames = {
            "transactions": f"belle0_transactions_{timestamp}.csv",
            "summary": f"belle0_summary_{timestamp}.csv",
            "clusters": f"belle0_clusters_{timestamp}.csv",
            "report": f"belle0_report_{timestamp}.txt"
        }
        saved = {}
        try:
            if not tx_df.empty:
                tx_df.to_csv(filenames["transactions"], index=False)
                logger.info(f"Saved transactions to {filenames['transactions']}")
                saved["transactions"] = filenames["transactions"]

            if not summary_df.empty:
                summary_df.to_csv(filenames["summary"], index=False)
                logger.info(f"Saved summary to {filenames['summary']}")
                saved["summary"] = filenames["summary"]

            if not cluster_df.empty:
                cluster_df.to_csv(filenames["clusters"], index=False)
                logger.info(f"Saved clusters to {filenames['clusters']}")
                saved["clusters"] = filenames["clusters"]

            report = self.generate_report(tx_df, summary_df, cluster_df)
            with open(filenames["report"], 'w', encoding='utf-8') as f:
                f.write(report)
            logger.info(f"Saved report to {filenames['report']}")
            saved["report"] = filenames["report"]
            return saved

        except Exception as e:
            logger.error(f"Error saving results: {e}")
            return saved

# -------------------- ë©”ì¸ --------------------
def main():
    config = Config()
    
    # 557ê°œ í™€ë” ë¶„ì„ì„ ìœ„í•œ ì„¤ì • í™•ì¸
    logger.info(f"=== ì„¤ì • í™•ì¸ ===")
    logger.info(f"ëª©í‘œ í™€ë” ìˆ˜: {config.HOLDER_LIMIT}ê°œ")
    logger.info(f"ë°°ì¹˜ í¬ê¸°: {config.BATCH_SIZE}")
    logger.info(f"ìš”ì²­ ì§€ì—°: {config.REQUEST_DELAY}ì´ˆ")
    logger.info(f"ì˜ˆìƒ ì†Œìš” ì‹œê°„: {config.HOLDER_LIMIT * config.REQUEST_DELAY / 60:.1f}ë¶„ ì´ìƒ")
    
    analyzer = EtherscanAnalyzer(
        api_key=os.getenv("ETHERSCAN_API_KEY", "YOUR_API_KEY"),
        config=config
    )

    token_addr = os.getenv("TOKEN_ADDRESS", "").strip()
    if not token_addr:
        raise SystemExit("í™˜ê²½ë³€ìˆ˜ TOKEN_ADDRESS ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì˜ˆ) set TOKEN_ADDRESS=0x...")

    # ìƒìœ„ 557ê°œ í™€ë” ìë™ ìˆ˜ì§‘
    logger.info(f"Fetching top {config.HOLDER_LIMIT} holders for token: {token_addr}")
    holders = analyzer.get_holders(token_addr, limit=config.HOLDER_LIMIT)
    if not holders:
        raise SystemExit("í™€ë” ëª©ë¡ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. TOKEN_ADDRESS/API í‚¤/ì¿¼í„°ë¥¼ ì ê²€í•˜ì„¸ìš”.")
    
    logger.info(f"Successfully fetched {len(holders)} holders")
    logger.info(f"First 10 holders: {holders[:10]}")
    logger.info(f"Last 10 holders: {holders[-10:]}")

    # ë¶„ì„ ì‹¤í–‰ (557ê°œ í™€ë” ì „ë¶€)
    logger.info("=== 557ê°œ í™€ë” ë¶„ì„ ì‹œì‘ ===")
    tx_df, summary_df, cluster_df = analyzer.analyze_multiple_holders(holders)

    # ì¶œë ¥
    print("\n" + analyzer.generate_report(tx_df, summary_df, cluster_df))

    if not summary_df.empty:
        print("\n=== ìƒì„¸ ìš”ì•½ (ìƒìœ„ 50ê°œ) ===")
        show = summary_df.sort_values(["tx_to_router", "fails"], ascending=False).head(50)
        print(show.to_string(index=False))

    if not tx_df.empty:
        print("\n=== ìµœê·¼ ì‹¤íŒ¨ ê±°ë˜ TOP 20 ===")
        recent_fails = tx_df[tx_df["status"] == "fail"].sort_values("time_utc", ascending=False).head(20)
        cols = ["holder", "time_utc", "method_guess", "gas_fee_eth"]
        print(recent_fails[cols].to_string(index=False))

    # ì €ì¥ (íŒŒì¼ëª…ì— 557 ì¶”ê°€)
    saved_files = analyzer.save_results(tx_df, summary_df, cluster_df)
    if saved_files:
        print(f"\nğŸ“ ì €ì¥ëœ íŒŒì¼ë“¤ (557ê°œ í™€ë” ë¶„ì„):")
        for k, v in saved_files.items():
            print(f"  - {k}: {v}")
    
    print("\n=== ë¶„ì„ ì™„ë£Œ ===")
    print(f"ì´ ë¶„ì„ í™€ë” ìˆ˜: {len(holders)}ê°œ")
    print(f"ì´ íŠ¸ëœì­ì…˜ ìˆ˜: {len(tx_df):,}ê°œ" if not tx_df.empty else "ì´ íŠ¸ëœì­ì…˜ ìˆ˜: 0ê°œ")

if __name__ == "__main__":
    main()
