# pip install requests pandas python-dateutil

import os
import time
import logging
import requests
import pandas as pd
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Optional, Tuple, Set
from dataclasses import dataclass
from requests.adapters import HTTPAdapter, Retry

# -------------------- ë¡œê¹… --------------------
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('belle_analysis.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

UTC = timezone.utc
KST = timezone(timedelta(hours=9))

# -------------------- ì„¤ì • --------------------
@dataclass
class Config:
    MIN_CLUSTER_SIZE: int = 3
    CLUSTER_WINDOW_MINUTES: int = 120
    HIGH_FAIL_RATE_THRESHOLD: float = 0.8
    CONSECUTIVE_FAIL_THRESHOLD: int = 10
    MAX_RETRIES: int = 3
    BATCH_SIZE: int = 1000          # txlist offset (<=1000 ê¶Œì¥)
    REQUEST_DELAY: float = 0.30     # 5 rps ì—¬ìœ 
    REQUEST_TIMEOUT: int = 30
    HOLDER_LIMIT: int = 335         # BELLE í™€ë” ìˆ˜ë¡œ ë³€ê²½
    USE_RECEIPT_GAS: bool = True    # receiptë¡œ ì •í™•í•œ ê°€ìŠ¤ë¹„ ê³„ì‚°
    ENRICH_RECEIPT_TOPN: int = 200  # receipt ë³´ì • ì‹¤íŒ¨ Tx ìµœëŒ€ ìˆ˜
    BELLE_TOKEN_ADDRESS: str = "0x34c6211621f2763c60eb007dc2ae91090a2d22f6"  # BELLE í† í° ì£¼ì†Œ

# -------------------- ìœ í‹¸ --------------------
def parse_window_from_env() -> Tuple[Optional[datetime], Optional[datetime], str]:
    """
    ì‹œê°„ ìœˆë„ìš°ë¥¼ í™˜ê²½ë³€ìˆ˜ì—ì„œ ì½ì–´ UTCë¡œ ë°˜í™˜.
    ìš°ì„ ìˆœìœ„:
      1) WINDOW_START_KST, WINDOW_END_KST  (KST ë¡œ í•´ì„)
      2) WINDOW_START_UTC, WINDOW_END_UTC  (UTC ë¡œ í•´ì„)
      3) ì—†ìŒ -> (None, None)
    í¬ë§·: YYYY-MM-DD HH:MM:SS
    """
    fmt = "%Y-%m-%d %H:%M:%S"
    ks = os.getenv("WINDOW_START_KST", "").strip()
    ke = os.getenv("WINDOW_END_KST", "").strip()
    us = os.getenv("WINDOW_START_UTC", "").strip()
    ue = os.getenv("WINDOW_END_UTC", "").strip()

    if ks and ke:
        try:
            start_kst = datetime.strptime(ks, fmt).replace(tzinfo=KST)
            end_kst   = datetime.strptime(ke, fmt).replace(tzinfo=KST)
            return start_kst.astimezone(UTC), end_kst.astimezone(UTC), "KST"
        except Exception as e:
            logger.warning(f"Failed to parse KST window: {e}")

    if us and ue:
        try:
            start_utc = datetime.strptime(us, fmt).replace(tzinfo=UTC)
            end_utc   = datetime.strptime(ue, fmt).replace(tzinfo=UTC)
            return start_utc, end_utc, "UTC"
        except Exception as e:
            logger.warning(f"Failed to parse UTC window: {e}")

    return None, None, ""

def within_window(ts_utc: datetime, start_utc: Optional[datetime], end_utc: Optional[datetime]) -> bool:
    if start_utc and ts_utc < start_utc:
        return False
    if end_utc and ts_utc > end_utc:
        return False
    return True

# -------------------- ë©”ì¸ í´ë˜ìŠ¤ --------------------
class EtherscanAnalyzer:
    def __init__(self, api_key: str, config: Config = None):
        # API í‚¤ ê²€ì¦ ê°œì„ 
        self.api_key = api_key or os.getenv("ETHERSCAN_API_KEY")
        if not self.api_key or self.api_key == "YOUR_API_KEY":
            raise ValueError("ETHERSCAN_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        self.config = config or Config()
        self.base_url = "https://api.etherscan.io/api"         # v1
        self.base_url_v2 = "https://api.etherscan.io/v2/api"   # v2 (beta)

        # ì„¸ì…˜ + Retry
        self.session = requests.Session()
        retries = Retry(
            total=5, backoff_factor=0.6,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["GET"]
        )
        self.session.mount("https://", HTTPAdapter(max_retries=retries))

        # ì‹œê°„ ìœˆë„ìš°(UTC) ì„¤ì •
        self.window_start_utc, self.window_end_utc, self.window_kind = parse_window_from_env()
        if self.window_start_utc or self.window_end_utc:
            s = self.window_start_utc.astimezone(KST).strftime("%Y-%m-%d %H:%M:%S %Z") if self.window_start_utc else "-"
            e = self.window_end_utc.astimezone(KST).strftime("%Y-%m-%d %H:%M:%S %Z") if self.window_end_utc else "-"
            logger.info(f"Applied time window [{self.window_kind}]: {s} ~ {e}")

        # ë¼ìš°í„° ì£¼ì†Œë“¤(BELLE í† í° ê±°ë˜ì—ì„œ ìì£¼ ì‚¬ìš©ë˜ëŠ” ë¼ìš°í„°ë“¤)
        self.routers: Set[str] = {
            "0x7a250d5630b4cf539739df2c5dacb4c659f2488d",  # Uniswap V2
            "0xe592427a0aece92de3edee1f18e0157c05861564",  # Uniswap V3
            "0x68b3465833fb72a70ecdf485e0e4c7bd8665fc45",  # Uniswap V3 Router 2
            "0x1111111254fb6c44bac0bed2854e76f90643097d",  # 1inch v4
            "0x1111111254eeb25477b68fb85ed929f73a960582",  # 1inch v5
            "0xdef1c0ded9bec7f1a1670819833240f027b25eff",  # 0x
            "0x881d40237659c251811cec9c364ef91dc08d300c",  # Metamask
        }
        # ì¶”ê°€ ë¼ìš°í„° ì§€ì›
        suspect_router = os.getenv("EXTRA_ROUTER", "").strip().lower()
        if suspect_router.startswith("0x") and len(suspect_router) == 42:
            self.routers.add(suspect_router)
            logger.info(f"Added EXTRA_ROUTER: {suspect_router}")

        # í•¨ìˆ˜ ì‹œê·¸ë‹ˆì²˜ ë§¤í•‘ (BELLE í† í° ê´€ë ¨ í™•ì¥)
        self.sig_map: Dict[str, str] = {
            # Uniswap V2
            "0x18cbafe5": "swapExactTokensForETH",                                   # SELL
            "0x791ac947": "swapExactTokensForETHSupportingFeeOnTransferTokens",      # SELL
            "0x4a25d94a": "swapTokensForExactETH",                                   # SELL
            "0x7ff36ab5": "swapExactETHForTokens",                                   # BUY
            "0xfb3bdb41": "swapETHForExactTokens",                                   # BUY
            "0x38ed1739": "swapExactTokensForTokens",
            "0x8803dbee": "swapTokensForExactTokens",
            "0xb6f9de95": "swapExactETHForTokensSupportingFeeOnTransferTokens",      # BUY
            # 1inch
            "0x12aa3caf": "swap",        # v5
            "0x0502b1c5": "unoswap",
            "0x2e95b6c8": "unoswap",
            # ì¶”ê°€ ì‹œê·¸ë‹ˆì²˜
            "0xa9059cbb": "transfer",    # ERC20 transfer
            "0x095ea7b3": "approve",     # ERC20 approve
        }

    # -------------------- í™€ë” ìˆ˜ì§‘ --------------------
    def get_top_holders_v2(self, token_addr: str, limit: int) -> List[str]:
        """v2 APIë¡œ ìƒìœ„ í™€ë” ê°€ì ¸ì˜¤ê¸°"""
        params = {
            "chainid": 1,
            "module": "token",
            "action": "topholders",
            "contractaddress": token_addr,
            "offset": min(limit, 1000),
            "apikey": self.api_key,
        }
        try:
            r = self.session.get(self.base_url_v2, params=params, timeout=self.config.REQUEST_TIMEOUT)
            r.raise_for_status()
            data = r.json()
            result = data.get("result", {})
            holders = result.get("holders", [])
            if isinstance(holders, list) and holders:
                return [h.get("address", "").lower() for h in holders if h.get("address")][:limit]
            if isinstance(result, list) and result:
                return [h.get("address", "").lower() for h in result if h.get("address")][:limit]
        except Exception as e:
            logger.warning(f"v2 topholders API failed: {e}")
        return []

    def get_holders_from_tokentx_v1(self, token_addr: str, limit: int) -> List[str]:
        """v1 API tokentxë¡œ í™€ë” ì¶”ì¶œ"""
        page = 1
        offset = min(10000, self.config.BATCH_SIZE)
        max_pages = min(50, 10000 // max(1, offset))  # BELLE í† í°ìš©ìœ¼ë¡œ í˜ì´ì§€ ì œí•œ ì™„í™”
        addrs: Set[str] = set()
        zero = "0x0000000000000000000000000000000000000000"
        token_addr_l = token_addr.lower()

        while page <= max_pages and len(addrs) < limit:
            params = {
                "module": "account",
                "action": "tokentx",
                "contractaddress": token_addr,
                "page": page,
                "offset": offset,
                "sort": "desc",  # ìµœì‹  ê±°ë˜ë¶€í„°
                "apikey": self.api_key
            }
            try:
                r = self.session.get(self.base_url, params=params, timeout=self.config.REQUEST_TIMEOUT)
                r.raise_for_status()
                data = r.json()
                if data.get("status") == "0" and "No transactions found" in data.get("message", ""):
                    break
                rows = data.get("result", []) or []
                if not rows:
                    break

                for t in rows:
                    frm = (t.get("from") or "").lower()
                    to = (t.get("to") or "").lower()
                    if frm and frm != zero and frm != token_addr_l:
                        addrs.add(frm)
                    if to and to != zero and to != token_addr_l:
                        addrs.add(to)
                    if len(addrs) >= limit:
                        break

                if len(rows) < offset:
                    break
                page += 1
                time.sleep(self.config.REQUEST_DELAY)
                
            except Exception as e:
                logger.warning(f"Error fetching tokentx page {page}: {e}")
                break

        res = list(addrs)[:limit]
        logger.info(f"Collected {len(res)} holder candidates from tokentx (v1)")
        return res

    def get_holders(self, token_addr: str, limit: int) -> List[str]:
        """í™€ë” ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (v2 ìš°ì„ , v1 í´ë°±)"""
        try:
            addrs = self.get_top_holders_v2(token_addr, limit)
            if len(addrs) >= min(limit // 2, 100):  # ë¶€ë¶„ì  ì„±ê³µë„ í—ˆìš©
                logger.info(f"Fetched {len(addrs)} top holders via v2")
                if len(addrs) < limit:
                    logger.info(f"v2ì—ì„œ {len(addrs)}ê°œë§Œ ìˆ˜ì§‘, v1ìœ¼ë¡œ ì¶”ê°€ ìˆ˜ì§‘ ì‹œë„")
                    additional = self.get_holders_from_tokentx_v1(token_addr, limit - len(addrs))
                    addrs.extend([addr for addr in additional if addr not in addrs])
                return addrs[:limit]
            logger.warning("v2 topholders returned insufficient data, falling back to v1 tokentx")
        except Exception as e:
            logger.warning(f"v2 topholders failed ({e}), falling back to v1 tokentx")
        return self.get_holders_from_tokentx_v1(token_addr, limit)

    # -------------------- íŠ¸ëœì­ì…˜ ìˆ˜ì§‘/ì •ê·œí™” --------------------
    def _validate_api_response(self, data: Dict) -> bool:
        """API ì‘ë‹µ ê²€ì¦"""
        if not isinstance(data, dict):
            return False
        if data.get("message") == "NOTOK":
            result = str(data.get("result", ""))
            if "rate limit" in result.lower():
                logger.warning("Rate limit hit, waiting...")
                time.sleep(2)  # ë ˆì´íŠ¸ ë¦¬ë°‹ ì‹œ ëŒ€ê¸°ì‹œê°„ ì¦ê°€
                return False
            if "no transactions found" in result.lower():
                return True  # ê±°ë˜ ì—†ìŒì€ ì •ìƒ ìƒí™©
            logger.warning(f"API returned NOTOK: {result}")
            return False
        return True

    def get_txlist(self, address: str, startblock: int = 0, endblock: int = 99999999, sort: str = "asc") -> List[Dict]:
        """íŠ¹ì • ì£¼ì†Œì˜ íŠ¸ëœì­ì…˜ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
        logger.info(f"Fetching transactions for address: {address}")
        page, offset = 1, self.config.BATCH_SIZE
        max_pages = min(100, 10000 // max(1, offset))
        all_rows: List[Dict] = []

        while True:
            if page > max_pages:
                logger.warning(f"Reached Etherscan window limit ({max_pages} pages) for {address}")
                break

            params = {
                "module": "account",
                "action": "txlist",
                "address": address,
                "startblock": startblock,
                "endblock": endblock,
                "page": page,
                "offset": offset,
                "sort": sort,
                "apikey": self.api_key
            }

            for attempt in range(self.config.MAX_RETRIES):
                try:
                    response = self.session.get(self.base_url, params=params, timeout=self.config.REQUEST_TIMEOUT)
                    response.raise_for_status()
                    data = response.json()
                    if not self._validate_api_response(data):
                        if attempt == self.config.MAX_RETRIES - 1:
                            logger.error(f"Failed after {self.config.MAX_RETRIES} attempts for {address}")
                            return all_rows
                        time.sleep(2 ** attempt)
                        continue

                    if data.get("status") == "0" and "No transactions found" in data.get("message", ""):
                        logger.info(f"No transactions found for {address}")
                        return all_rows

                    batch = data.get("result", []) or []
                    if not isinstance(batch, list):
                        return all_rows
                    if not batch:
                        return all_rows

                    all_rows.extend(batch)
                    logger.info(f"Fetched {len(batch)} transactions (page {page}) for {address}")

                    if len(batch) < offset:
                        return all_rows

                    page += 1
                    time.sleep(max(self.config.REQUEST_DELAY, 0.25))
                    break

                except requests.RequestException as e:
                    logger.warning(f"Request attempt {attempt + 1} failed for {address}: {e}")
                    if attempt == self.config.MAX_RETRIES - 1:
                        logger.error(f"All retry attempts failed for {address}")
                        return all_rows
                    time.sleep(2 ** attempt)
            else:
                break

        logger.info(f"Total {len(all_rows)} transactions fetched for {address}")
        return all_rows

    def _is_sell(self, method_name: str) -> bool:
        """ë§¤ë„ ê±°ë˜ì¸ì§€ íŒë‹¨"""
        if not method_name:
            return False
        name = method_name.lower()
        return ("tokensforeth" in name)  # swapExactTokensForETH / swapTokensForExactETH ë“±

    def normalize_tx_rows(self, rows: List[Dict], holder: str) -> pd.DataFrame:
        """íŠ¸ëœì­ì…˜ ë°ì´í„° ì •ê·œí™”"""
        if not rows:
            return pd.DataFrame()

        processed_rows = []
        for tx in rows:
            try:
                if not all(k in tx for k in ["hash", "timeStamp", "blockNumber", "to"]):
                    continue

                to_addr = (tx.get("to") or "").lower()
                if to_addr not in self.routers:
                    continue  # ë¼ìš°í„° í˜¸ì¶œë§Œ

                # ì‹œê°„ íŒŒì‹± (UTC aware) + ìœˆë„ìš° í•„í„°
                ts = int(tx["timeStamp"])
                time_utc = datetime.fromtimestamp(ts, UTC)
                if not within_window(time_utc, self.window_start_utc, self.window_end_utc):
                    continue  # ì‹œê°„ ìœˆë„ìš° ì ìš©

                input_data = tx.get("input", "") or ""
                method_sig = input_data[:10].lower() if (input_data.startswith("0x") and len(input_data) >= 10) else ""
                method_name = self.sig_map.get(method_sig, f"sig:{method_sig}" if method_sig else "unknown")

                is_error = str(tx.get("isError", "0")) == "1"
                receipt_failed = str(tx.get("txreceipt_status", "1")) == "0"
                status = "fail" if (is_error or receipt_failed) else "success"

                block_num = int(tx["blockNumber"])
                gas_used = int(tx.get("gasUsed", 0))
                gas_price = int(tx.get("gasPrice", 0))
                value_eth = float(tx.get("value", 0)) / 1e18
                gas_fee_eth = (gas_used * gas_price) / 1e18

                processed_rows.append({
                    "holder": holder.lower(),
                    "hash": tx["hash"],
                    "time_utc": time_utc,
                    "block": block_num,
                    "to": to_addr,
                    "method_guess": method_name,
                    "is_sell_path": self._is_sell(method_name),
                    "value_eth": value_eth,
                    "gas_used": gas_used,
                    "gas_price_gwei": gas_price / 1e9 if gas_price else 0.0,
                    "gas_fee_eth": gas_fee_eth,
                    "isError": tx.get("isError", "0"),
                    "txreceipt_status": tx.get("txreceipt_status", "1"),
                    "status": status,
                })
            except Exception as e:
                logger.warning(f"Error processing tx {tx.get('hash', '')}: {e}")
                continue

        df = pd.DataFrame(processed_rows)
        if not df.empty:
            df = df.drop_duplicates("hash", keep="first")
        logger.info(f"Processed {len(df)} router transactions in window for {holder}")
        return df

    # -------------------- ë¶„ì„ --------------------
    def _enrich_gas_with_receipts(self, df: pd.DataFrame) -> None:
        """Receiptë¡œ ì •í™•í•œ ê°€ìŠ¤ë¹„ ê³„ì‚°"""
        if df.empty:
            return
        # ì‹¤íŒ¨í•œ ê±°ë˜ ì¤‘ ìµœê·¼ ê²ƒë“¤ì— ëŒ€í•´ì„œë§Œ receipt ì¡°íšŒ
        target = df[df["status"] == "fail"].sort_values("time_utc", ascending=False).head(self.config.ENRICH_RECEIPT_TOPN)
        for h in target["hash"].tolist():
            try:
                r = self.session.get(self.base_url, params={
                    "module": "proxy",
                    "action": "eth_getTransactionReceipt",
                    "txhash": h,
                    "apikey": self.api_key
                }, timeout=self.config.REQUEST_TIMEOUT)
                js = r.json()
                result = js.get("result") or {}
                eff = result.get("effectiveGasPrice")  # EIP-1559 ì´í›„
                gas_used_hex = result.get("gasUsed")
                if eff and gas_used_hex:
                    eff_int = int(eff, 16)
                    used = int(gas_used_hex, 16)
                    df.loc[df["hash"] == h, "gas_price_gwei"] = eff_int / 1e9
                    df.loc[df["hash"] == h, "gas_fee_eth"] = (eff_int * used) / 1e18
                time.sleep(0.1)  # Receipt ì¡°íšŒ ê°„ ì§§ì€ ëŒ€ê¸°
            except Exception:
                pass

    def cluster_failures(self, df: pd.DataFrame, window_minutes: Optional[int] = None) -> pd.DataFrame:
        """ì‹¤íŒ¨ ê±°ë˜ í´ëŸ¬ìŠ¤í„°ë§"""
        if df.empty:
            return pd.DataFrame(columns=["holder", "start", "end", "count", "gas_waste_eth"])
        window_minutes = window_minutes or self.config.CLUSTER_WINDOW_MINUTES
        fails = df[df["status"] == "fail"].sort_values("time_utc").copy()
        if fails.empty:
            return pd.DataFrame(columns=["holder", "start", "end", "count", "gas_waste_eth"])

        clusters = []
        i = 0
        while i < len(fails):
            start_time = fails.iloc[i]["time_utc"]
            start_idx = i
            while i + 1 < len(fails):
                next_time = fails.iloc[i + 1]["time_utc"]
                if (next_time - start_time).total_seconds() <= window_minutes * 60:
                    i += 1
                else:
                    break
            end_idx = i
            count = end_idx - start_idx + 1
            if count >= self.config.MIN_CLUSTER_SIZE:
                cluster_txs = fails.iloc[start_idx:end_idx + 1]
                gas_waste = cluster_txs["gas_fee_eth"].sum()
                clusters.append({
                    "holder": fails.iloc[start_idx]["holder"],
                    "start": start_time,
                    "end": fails.iloc[end_idx]["time_utc"],
                    "count": count,
                    "gas_waste_eth": gas_waste
                })
            i += 1
        return pd.DataFrame(clusters)

    def detect_honeypot_indicators(self, summary_df: pd.DataFrame) -> Dict[str, List[str]]:
        """í—ˆë‹ˆíŒŸ ì§€í‘œ íƒì§€"""
        indicators = {}
        for _, row in summary_df.iterrows():
            holder = row['holder']
            holder_indicators = []
            
            # BELLE í† í°ì— íŠ¹í™”ëœ ì§€í‘œ
            if row['fail_rate'] >= self.config.HIGH_FAIL_RATE_THRESHOLD:
                holder_indicators.append(f"ë†’ì€ ì‹¤íŒ¨ìœ¨: {row['fail_rate']:.1%}")
            if row['max_consecutive_fails'] >= self.config.CONSECUTIVE_FAIL_THRESHOLD:
                holder_indicators.append(f"ì—°ì† ì‹¤íŒ¨: {row['max_consecutive_fails']}íšŒ")
            if row['clusters_2h_count'] >= 3:
                holder_indicators.append(f"ì‹¤íŒ¨ í´ëŸ¬ìŠ¤í„°: {row['clusters_2h_count']}ê°œ")
            if row['total_gas_waste_eth'] > 0.05:  # BELLEìš©ìœ¼ë¡œ ì„ê³„ê°’ ë‚®ì¶¤
                holder_indicators.append(f"ê°€ìŠ¤ ë‚­ë¹„: {row['total_gas_waste_eth']:.3f} ETH")
            if 'sell_fail_rate' in row and row['sell_txs'] >= 3 and row['sell_fail_rate'] >= 0.7:  # ë§¤ë„ ì‹¤íŒ¨ ê¸°ì¤€ ì™„í™”
                holder_indicators.append(f"ë§¤ë„ ì‹¤íŒ¨ìœ¨ ë†’ìŒ: {row['sell_fail_rate']:.1%} / {row['sell_txs']}ê±´")
                
            if holder_indicators:
                indicators[holder] = holder_indicators
        return indicators

    def analyze_holder(self, address: str) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """í™€ë” ê°œë³„ ë¶„ì„"""
        logger.info(f"Starting analysis for holder: {address}")
        raw_txs = self.get_txlist(address)
        df = self.normalize_tx_rows(raw_txs, address)

        if df.empty:
            empty_summary = pd.DataFrame([{
                "holder": address.lower(),
                "tx_to_router": 0,
                "fails": 0,
                "success": 0,
                "fail_rate": 0.0,
                "max_consecutive_fails": 0,
                "avg_gas_failed": 0.0,
                "total_gas_waste_eth": 0.0,
                "clusters_2h_count": 0,
                "sell_txs": 0,
                "sell_fail_rate": 0.0,
                "first_tx": None,
                "last_tx": None
            }])
            return df, empty_summary, pd.DataFrame()

        # Receiptë¡œ ê°€ìŠ¤ë¹„ ë³´ì • (BELLE í† í° ë¶„ì„ì—ì„œëŠ” ê¸°ë³¸ í™œì„±í™”)
        if self.config.USE_RECEIPT_GAS:
            self._enrich_gas_with_receipts(df)

        df = df.sort_values("time_utc").reset_index(drop=True)

        total_txs = len(df)
        failed_txs = int((df["status"] == "fail").sum())
        success_txs = total_txs - failed_txs
        fail_rate = failed_txs / total_txs if total_txs else 0.0

        # ì—°ì† ì‹¤íŒ¨ ê³„ì‚°
        consecutive_fails = 0
        max_consecutive_fails = 0
        for status in df["status"]:
            if status == "fail":
                consecutive_fails += 1
                max_consecutive_fails = max(max_consecutive_fails, consecutive_fails)
            else:
                consecutive_fails = 0

        # ê°€ìŠ¤ë¹„ ë¶„ì„
        failed_df = df[df["status"] == "fail"]
        avg_gas_failed = float(failed_df["gas_used"].mean()) if not failed_df.empty else 0.0
        total_gas_waste = float(failed_df["gas_fee_eth"].sum()) if not failed_df.empty else 0.0

        # ë§¤ë„ ê±°ë˜ ë¶„ì„
        sell_df = df[df["is_sell_path"] == True]
        sell_fail_rate = float((sell_df["status"] == "fail").mean()) if not sell_df.empty else 0.0

        # ì‹¤íŒ¨ í´ëŸ¬ìŠ¤í„° ë¶„ì„
        clusters = self.cluster_failures(df)

        summary = pd.DataFrame([{
            "holder": address.lower(),
            "tx_to_router": total_txs,
            "fails": failed_txs,
            "success": success_txs,
            "fail_rate": round(fail_rate, 4),
            "max_consecutive_fails": int(max_consecutive_fails),
            "avg_gas_failed": round(avg_gas_failed, 0),
            "total_gas_waste_eth": round(total_gas_waste, 6),
            "clusters_2h_count": len(clusters),
            "sell_txs": int(len(sell_df)),
            "sell_fail_rate": round(sell_fail_rate, 4),
            "first_tx": df["time_utc"].min(),
            "last_tx": df["time_utc"].max()
        }])

        logger.info(f"Analysis complete for {address}: {total_txs} txs (window), {fail_rate:.1%} fail rate")
        return df, summary, clusters

    def safe_concat_dataframes(self, df_list: List[pd.DataFrame], name: str) -> pd.DataFrame:
        """ì•ˆì „í•œ ë°ì´í„°í”„ë ˆì„ ê²°í•©"""
        if not df_list:
            logger.warning(f"No dataframes to concat for {name}")
            return pd.DataFrame()
        try:
            result = pd.concat(df_list, ignore_index=True)
            logger.info(f"Successfully concatenated {len(df_list)} dataframes for {name}")
            return result
        except Exception as e:
            logger.error(f"Error concatenating dataframes for {name}: {e}")
            return pd.DataFrame()

    def analyze_multiple_holders(self, holders: List[str]) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """ë‹¤ì¤‘ í™€ë” ë¶„ì„"""
        logger.info(f"Starting analysis for {len(holders)} BELLE holders")
        all_transactions, all_summaries, all_clusters = [], [], []

        for i, holder in enumerate(holders, 1):
            logger.info(f"Processing BELLE holder {i}/{len(holders)}: {holder}")
            try:
                tx_df, summary_df, cluster_df = self.analyze_holder(holder)
                if not tx_df.empty:
                    all_transactions.append(tx_df)
                if not summary_df.empty:
                    all_summaries.append(summary_df)
                if not cluster_df.empty:
                    all_clusters.append(cluster_df)
                    
                # ì§„í–‰ë¥  ë¡œê·¸ (BELLE 335ê°œ í™€ë”ìš©)
                if i % 25 == 0 or i == len(holders):
                    progress = (i / len(holders)) * 100
                    logger.info(f"Progress: {i}/{len(holders)} ({progress:.1f}%) completed")
                    
            except Exception as e:
                logger.error(f"Error analyzing BELLE holder {holder}: {e}")
                continue

        combined_txs = self.safe_concat_dataframes(all_transactions, "transactions")
        combined_summaries = self.safe_concat_dataframes(all_summaries, "summaries")
        combined_clusters = self.safe_concat_dataframes(all_clusters, "clusters")
        return combined_txs, combined_summaries, combined_clusters

    # -------------------- ì¶œë ¥/ì €ì¥ --------------------
    def generate_report(self, tx_df: pd.DataFrame, summary_df: pd.DataFrame, cluster_df: pd.DataFrame) -> str:
        """BELLE í† í° ë¶„ì„ ë³´ê³ ì„œ ìƒì„±"""
        report = []
        report.append("=" * 80)
        report.append("BELLE í† í° í—ˆë‹ˆíŒŸ ë¶„ì„ ë³´ê³ ì„œ (335ê°œ í™€ë”)")
        report.append(f"í† í° ì£¼ì†Œ: {self.config.BELLE_TOKEN_ADDRESS}")
        report.append("=" * 80)

        # í˜„ì¬ ì‹œê° ë° ìœˆë„ìš° í‘œê¸°
        report.append(f"ë¶„ì„ ì‹œê°„: {datetime.now(UTC).astimezone(KST).strftime('%Y-%m-%d %H:%M:%S %Z')}")
        if self.window_start_utc or self.window_end_utc:
            s = self.window_start_utc.astimezone(KST).strftime('%Y-%m-%d %H:%M:%S %Z') if self.window_start_utc else "-"
            e = self.window_end_utc.astimezone(KST).strftime('%Y-%m-%d %H:%M:%S %Z') if self.window_end_utc else "-"
            report.append(f"ì ìš© ìœˆë„ìš°: {s} ~ {e}")
        else:
            report.append("ì ìš© ìœˆë„ìš°: ì „ì²´ ê¸°ê°„")
        report.append("")

        if summary_df.empty:
            report.append("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return "\n".join(report)

        total_holders = len(summary_df)
        holders_with_txs = len(summary_df[summary_df["tx_to_router"] > 0])
        total_txs = int(summary_df["tx_to_router"].sum())
        total_fails = int(summary_df["fails"].sum())
        overall_fail_rate = (total_fails / total_txs) if total_txs > 0 else 0.0
        total_gas_waste = float(summary_df["total_gas_waste_eth"].sum())

        report.append("ğŸ“Š ì „ì²´ í†µê³„:")
        report.append(f"  - ë¶„ì„ í™€ë” ìˆ˜: {total_holders} / 335 (ëª©í‘œ)")
        report.append(f"  - ê±°ë˜ í™œë™ í™€ë”: {holders_with_txs}")
        report.append(f"  - ì´ ë¼ìš°í„° ê±°ë˜(ìœˆë„ìš° ë‚´): {total_txs:,}")
        report.append(f"  - ì‹¤íŒ¨ ê±°ë˜: {total_fails:,}")
        report.append(f"  - ì „ì²´ ì‹¤íŒ¨ìœ¨: {overall_fail_rate:.1%}")
        report.append(f"  - ì´ ê°€ìŠ¤ ë‚­ë¹„: {total_gas_waste:.4f} ETH")
        report.append("")

        # í—ˆë‹ˆíŒŸ ì§€í‘œ íƒì§€
        indicators = self.detect_honeypot_indicators(summary_df)
        high_risk_holders = len([h for h, inds in indicators.items() if len(inds) >= 2])
        
        if indicators:
            report.append("ğŸš¨ í—ˆë‹ˆíŒŸ ì§€í‘œ íƒì§€:")
            report.append(f"  - ì§€í‘œ ë³´ìœ  í™€ë”: {len(indicators)}ëª…")
            report.append(f"  - ê³ ìœ„í—˜ í™€ë” (2ê°œ ì´ìƒ ì§€í‘œ): {high_risk_holders}ëª…")
            report.append("")
            
            # ìƒìœ„ 10ê°œ ë¬¸ì œ í™€ë”ë§Œ í‘œì‹œ
            sorted_indicators = sorted(indicators.items(), key=lambda x: len(x[1]), reverse=True)
            for i, (holder, holder_indicators) in enumerate(sorted_indicators[:10], 1):
                report.append(f"  [{i}] {holder[:10]}...:")
                for indicator in holder_indicators:
                    report.append(f"    âš ï¸  {indicator}")
            if len(indicators) > 10:
                report.append(f"  ... ë° {len(indicators) - 10}ëª… ì¶”ê°€")
            report.append("")

        # ë§¤ë„ ì‹¤íŒ¨ ë¶„ì„ (BELLE íŠ¹í™”)
        sell_analysis = summary_df[summary_df["sell_txs"] >= 3]
        if not sell_analysis.empty:
            high_sell_fail = sell_analysis[sell_analysis["sell_fail_rate"] >= 0.7]
            report.append("ğŸ”´ ë§¤ë„ ì‹¤íŒ¨ ë¶„ì„:")
            report.append(f"  - ë§¤ë„ ì‹œë„ í™€ë” (3íšŒ ì´ìƒ): {len(sell_analysis)}ëª…")
            report.append(f"  - ë§¤ë„ ì‹¤íŒ¨ìœ¨ 70% ì´ìƒ: {len(high_sell_fail)}ëª…")
            if not high_sell_fail.empty:
                avg_sell_fail_rate = high_sell_fail["sell_fail_rate"].mean()
                report.append(f"  - í‰ê·  ë§¤ë„ ì‹¤íŒ¨ìœ¨: {avg_sell_fail_rate:.1%}")
            report.append("")

        # í´ëŸ¬ìŠ¤í„° ë¶„ì„
        if not cluster_df.empty:
            total_clusters = len(cluster_df)
            avg_cluster_size = cluster_df["count"].mean()
            total_cluster_waste = cluster_df["gas_waste_eth"].sum()
            report.append("ğŸ“ˆ ì‹¤íŒ¨ í´ëŸ¬ìŠ¤í„° ë¶„ì„:")
            report.append(f"  - ì´ í´ëŸ¬ìŠ¤í„°: {total_clusters}ê°œ")
            report.append(f"  - í‰ê·  í´ëŸ¬ìŠ¤í„° í¬ê¸°: {avg_cluster_size:.1f}íšŒ")
            report.append(f"  - í´ëŸ¬ìŠ¤í„° ê°€ìŠ¤ ë‚­ë¹„: {total_cluster_waste:.4f} ETH")
            report.append("")

        # ê²°ë¡  (BELLE í† í° íŠ¹í™”)
        report.append("ğŸ“‹ BELLE í† í° ë¶„ì„ ê²°ë¡ :")
        
        # ê¸°ì¡´ ì •ë³´: ìˆ˜ë™ ê³„ì‚°ìœ¼ë¡œ 38ê°œ revert í™•ì¸
        report.append(f"  ğŸ“Œ ì°¸ê³ : ìˆ˜ë™ ê³„ì‚°ìœ¼ë¡œ ì•½ 38ê°œ ì§€ê°‘ì—ì„œ revert í™•ì¸ë¨")
        report.append("")
        
        if overall_fail_rate >= 0.6:
            report.append("  ğŸ”´ HIGH RISK: ë§¤ìš° ë†’ì€ ì‹¤íŒ¨ìœ¨")
        elif overall_fail_rate >= 0.3:
            report.append("  ğŸŸ¡ MEDIUM RISK: ì¤‘ê°„ ìˆ˜ì¤€ ì‹¤íŒ¨ìœ¨")
        else:
            report.append("  ğŸŸ¢ LOW RISK: ë‚®ì€ ì‹¤íŒ¨ìœ¨")
            
        if high_risk_holders >= 10:
            report.append("  ğŸ”´ HIGH RISK: ë‹¤ìˆ˜ í™€ë”ì—ì„œ í—ˆë‹ˆíŒŸ ì§€í‘œ ë°œê²¬")
        elif high_risk_holders >= 3:
            report.append("  ğŸŸ¡ MEDIUM RISK: ì¼ë¶€ í™€ë”ì—ì„œ í—ˆë‹ˆíŒŸ ì§€í‘œ ë°œê²¬")
            
        if len(high_sell_fail) >= 5:
            report.append("  ğŸ”´ HIGH RISK: ë‹¤ìˆ˜ í™€ë”ì˜ ë§¤ë„ ì‹œë„ ì‹¤íŒ¨")
            
        # ìµœì¢… íŒì •
        risk_factors = 0
        if overall_fail_rate >= 0.5: risk_factors += 1
        if high_risk_holders >= 5: risk_factors += 1
        if len(high_sell_fail) >= 3: risk_factors += 1
        if total_gas_waste >= 1.0: risk_factors += 1
        
        report.append("")
        if risk_factors >= 3:
            report.append("  ğŸš¨ ê²°ë¡ : BELLEëŠ” í—ˆë‹ˆíŒŸ ìŠ¤ìº  í† í°ìœ¼ë¡œ ê°•ë ¥íˆ ì˜ì‹¬ë¨")
            report.append("      â†’ ë§¤ë„ ì°¨ë‹¨ ë©”ì»¤ë‹ˆì¦˜ ì¡´ì¬ ê°€ëŠ¥ì„± ë†’ìŒ")
        elif risk_factors >= 2:
            report.append("  âš ï¸  ê²°ë¡ : BELLEëŠ” í—ˆë‹ˆíŒŸ ê°€ëŠ¥ì„±ì´ ìˆëŠ” ìœ„í—˜í•œ í† í°")
            report.append("      â†’ íˆ¬ì ì‹œ ê·¹ë„ì˜ ì£¼ì˜ í•„ìš”")
        else:
            report.append("  ğŸ“Š ê²°ë¡ : ì¼ë¶€ ì§€í‘œ ì¡´ì¬í•˜ë‚˜ ê²°ì •ì  ì¦ê±° ë¶€ì¡±")
            report.append("      â†’ ì¶”ê°€ ëª¨ë‹ˆí„°ë§ ê¶Œì¥")

        report.append("=" * 80)
        return "\n".join(report)

    def save_results(self, tx_df: pd.DataFrame, summary_df: pd.DataFrame, cluster_df: pd.DataFrame) -> Dict[str, str]:
        """ê²°ê³¼ íŒŒì¼ ì €ì¥"""
        timestamp = datetime.now(UTC).strftime("%Y%m%d_%H%M%S")
        filenames = {
            "transactions": f"belle_transactions_335_{timestamp}.csv",
            "summary": f"belle_summary_335_{timestamp}.csv",
            "clusters": f"belle_clusters_335_{timestamp}.csv",
            "report": f"belle_report_335_{timestamp}.txt"
        }
        saved = {}
        try:
            if not tx_df.empty:
                tx_df.to_csv(filenames["transactions"], index=False, encoding='utf-8-sig')
                logger.info(f"Saved transactions to {filenames['transactions']}")
                saved["transactions"] = filenames["transactions"]

            if not summary_df.empty:
                summary_df.to_csv(filenames["summary"], index=False, encoding='utf-8-sig')
                logger.info(f"Saved summary to {filenames['summary']}")
                saved["summary"] = filenames["summary"]

            if not cluster_df.empty:
                cluster_df.to_csv(filenames["clusters"], index=False, encoding='utf-8-sig')
                logger.info(f"Saved clusters to {filenames['clusters']}")
                saved["clusters"] = filenames["clusters"]

            report = self.generate_report(tx_df, summary_df, cluster_df)
            with open(filenames["report"], 'w', encoding='utf-8') as f:
                f.write(report)
            logger.info(f"Saved report to {filenames['report']}")
            saved["report"] = filenames["report"]
            return saved

        except Exception as e:
            logger.error(f"Error saving results: {e}")
            return saved

# -------------------- ë©”ì¸ --------------------
def main():
    """BELLE í† í° ë¶„ì„ ë©”ì¸ í•¨ìˆ˜"""
    config = Config()
    
    # BELLE í† í° ë¶„ì„ ì„¤ì • í™•ì¸
    logger.info(f"=== BELLE í† í° ë¶„ì„ ì„¤ì • ===")
    logger.info(f"í† í° ì£¼ì†Œ: {config.BELLE_TOKEN_ADDRESS}")
    logger.info(f"ëª©í‘œ í™€ë” ìˆ˜: {config.HOLDER_LIMIT}ê°œ")
    logger.info(f"ë°°ì¹˜ í¬ê¸°: {config.BATCH_SIZE}")
    logger.info(f"ìš”ì²­ ì§€ì—°: {config.REQUEST_DELAY}ì´ˆ")
    logger.info(f"Receipt ê°€ìŠ¤ë¹„ ë³´ì •: {config.USE_RECEIPT_GAS}")
    logger.info(f"ì˜ˆìƒ ì†Œìš” ì‹œê°„: {config.HOLDER_LIMIT * config.REQUEST_DELAY / 60:.1f}ë¶„ ì´ìƒ")
    
    try:
        analyzer = EtherscanAnalyzer(
            api_key=os.getenv("ETHERSCAN_API_KEY"),
            config=config
        )
    except ValueError as e:
        logger.error(f"ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        print(f"\nâŒ ì—ëŸ¬: {e}")
        print("PowerShellì—ì„œ ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ API í‚¤ë¥¼ ì„¤ì •í•˜ì„¸ìš”:")
        print('$env:ETHERSCAN_API_KEY = "ì—¬ê¸°ì—_ì‹¤ì œ_API_í‚¤_ì…ë ¥"')
        return

    # í† í° ì£¼ì†ŒëŠ” í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” ê¸°ë³¸ê°’(BELLE) ì‚¬ìš©
    token_addr = os.getenv("TOKEN_ADDRESS", config.BELLE_TOKEN_ADDRESS).strip()
    if not token_addr:
        logger.error("TOKEN_ADDRESSê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        return

    logger.info(f"ë¶„ì„ ëŒ€ìƒ: {token_addr}")

    # BELLE í† í°ì˜ 335ê°œ í™€ë” ìë™ ìˆ˜ì§‘
    logger.info(f"Fetching top {config.HOLDER_LIMIT} holders for BELLE token: {token_addr}")
    holders = analyzer.get_holders(token_addr, limit=config.HOLDER_LIMIT)
    if not holders:
        logger.error("í™€ë” ëª©ë¡ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. TOKEN_ADDRESS/API í‚¤/ì¿¼í„°ë¥¼ ì ê²€í•˜ì„¸ìš”.")
        print("\nâŒ í™€ë” ëª©ë¡ ìˆ˜ì§‘ ì‹¤íŒ¨")
        print("ë‹¤ìŒ ì‚¬í•­ì„ í™•ì¸í•˜ì„¸ìš”:")
        print("1. ETHERSCAN_API_KEYê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì—ˆëŠ”ì§€")
        print("2. API í‚¤ì˜ ì¿¼í„°ê°€ ë‚¨ì•„ìˆëŠ”ì§€")
        print("3. í† í° ì£¼ì†Œê°€ ì˜¬ë°”ë¥¸ì§€")
        return
    
    logger.info(f"Successfully fetched {len(holders)} BELLE holders")
    print(f"\nâœ… {len(holders)}ê°œ í™€ë” ìˆ˜ì§‘ ì™„ë£Œ")
    print(f"First 5 holders: {holders[:5]}")

    # ë¶„ì„ ì‹¤í–‰ (BELLE 335ê°œ í™€ë”)
    print(f"\nğŸ” BELLE í† í° 335ê°œ í™€ë” ë¶„ì„ ì‹œì‘...")
    print(f"ì˜ˆìƒ ì†Œìš” ì‹œê°„: {len(holders) * config.REQUEST_DELAY / 60:.1f}ë¶„")
    print("ë¶„ì„ ì§„í–‰ ì¤‘... (ë¡œê·¸ íŒŒì¼ì—ì„œ ìƒì„¸ ì§„í–‰ìƒí™© í™•ì¸ ê°€ëŠ¥)")
    
    tx_df, summary_df, cluster_df = analyzer.analyze_multiple_holders(holders)

    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*60)
    print(analyzer.generate_report(tx_df, summary_df, cluster_df))

    # ìƒì„¸ ë¶„ì„ ê²°ê³¼
    if not summary_df.empty:
        print("\n=== ìƒìœ„ ìœ„í—˜ í™€ë” (ì‹¤íŒ¨ìœ¨ ê¸°ì¤€) ===")
        risky = summary_df[summary_df["fail_rate"] >= 0.3].sort_values(["fail_rate", "total_gas_waste_eth"], ascending=False)
        if not risky.empty:
            display_cols = ["holder", "tx_to_router", "fails", "fail_rate", "sell_fail_rate", "total_gas_waste_eth", "clusters_2h_count"]
            print(risky[display_cols].head(20).to_string(index=False))
        else:
            print("ì‹¤íŒ¨ìœ¨ 30% ì´ìƒì¸ í™€ë”ê°€ ì—†ìŠµë‹ˆë‹¤.")

    if not tx_df.empty:
        print("\n=== ìµœê·¼ ì‹¤íŒ¨ ê±°ë˜ ë¶„ì„ ===")
        recent_fails = tx_df[tx_df["status"] == "fail"].sort_values("time_utc", ascending=False).head(15)
        if not recent_fails.empty:
            cols = ["holder", "time_utc", "method_guess", "gas_fee_eth", "is_sell_path"]
            print(recent_fails[cols].to_string(index=False))

    # íŒŒì¼ ì €ì¥
    saved_files = analyzer.save_results(tx_df, summary_df, cluster_df)
    if saved_files:
        print(f"\nğŸ“ ì €ì¥ëœ íŒŒì¼ë“¤:")
        for k, v in saved_files.items():
            print(f"  - {k}: {v}")
    
    print(f"\nâœ… BELLE í† í° ë¶„ì„ ì™„ë£Œ")
    print(f"ì´ ë¶„ì„ í™€ë” ìˆ˜: {len(holders)}ê°œ")
    print(f"ì´ íŠ¸ëœì­ì…˜ ìˆ˜: {len(tx_df):,}ê°œ" if not tx_df.empty else "ì´ íŠ¸ëœì­ì…˜ ìˆ˜: 0ê°œ")
    print(f"ìˆ˜ë™ ê³„ì‚° ëŒ€ë¹„: ìë™ ë¶„ì„ì—ì„œ íƒì§€ëœ ë¬¸ì œ í™€ë”ì™€ ê¸°ì¡´ 38ê°œ revert í™€ë” ë¹„êµ ë¶„ì„ ì™„ë£Œ")

if __name__ == "__main__":
    main()