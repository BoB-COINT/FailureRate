# pip install requests pandas python-dateutil

import os
import sys
import time
import random
import logging
import requests
import pandas as pd
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Optional, Tuple, Set
from dataclasses import dataclass
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry  # âœ… ì˜¬ë°”ë¥¸ ìœ„ì¹˜ì—ì„œ ì„í¬íŠ¸

# -------------------- ë¡œê¹… --------------------
def setup_logging(token_name: str) -> logging.Logger:
    """í† í°ëª… ê¸°ë°˜ ë¡œê¹… ì„¤ì • (ì½˜ì†”+íŒŒì¼)"""
    log_filename = f'{(token_name or "token").lower()}_analysis.log'

    # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±°
    for h in logging.root.handlers[:]:
        logging.root.removeHandler(h)

    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_filename, encoding='utf-8'),
            logging.StreamHandler(sys.stdout),
        ],
        force=True
    )
    return logging.getLogger(token_name.lower() or __name__)

UTC = timezone.utc
KST = timezone(timedelta(hours=9))
_modlog = logging.getLogger(__name__)  # ëª¨ë“ˆ ìœ í‹¸ìš© ë¡œê±°(í´ë˜ìŠ¤ ë°–)

# -------------------- ì„¤ì • --------------------
@dataclass
class Config:
    # ë¶„ì„ íŒŒë¼ë¯¸í„°
    MIN_CLUSTER_SIZE: int = 3
    CLUSTER_WINDOW_MINUTES: int = 120
    HIGH_FAIL_RATE_THRESHOLD: float = 0.8
    CONSECUTIVE_FAIL_THRESHOLD: int = 10

    # ë„¤íŠ¸ì›Œí‚¹/ì„±ëŠ¥
    MAX_RETRIES: int = 3
    BATCH_SIZE: int = 1000
    REQUEST_DELAY: float = 0.25       # rps 4~5 ê¶Œì¥
    REQUEST_TIMEOUT: int = 15
    USE_RECEIPT_GAS: bool = True      # ì‹¤íŒ¨ Top-Nì€ receiptë¡œ ê°€ìŠ¤ ë³´ì •
    ENRICH_RECEIPT_TOPN: int = 100

    # NOTE: HOLDER_LIMIT, TOKEN_ADDRESS ë“±ì€ ì½”ë“œì— í•˜ë“œì½”ë”©í•˜ì§€ ì•ŠìŒ(í™˜ê²½ë³€ìˆ˜ í•„ìˆ˜)
    HOLDER_LIMIT: Optional[int] = None

# -------------------- ìœ í‹¸ --------------------
def parse_window_from_env() -> Tuple[Optional[datetime], Optional[datetime], str]:
    """
    ì‹œê°„ ìœˆë„ìš°ë¥¼ í™˜ê²½ë³€ìˆ˜ì—ì„œ ì½ì–´ UTCë¡œ ë°˜í™˜.
      - WINDOW_START_KST / WINDOW_END_KST  (KST í•´ì„)
      - WINDOW_START_UTC / WINDOW_END_UTC  (UTC í•´ì„)
      - ì—†ìœ¼ë©´ (None, None, "")
    """
    fmt = "%Y-%m-%d %H:%M:%S"
    ks = os.getenv("WINDOW_START_KST", "").strip()
    ke = os.getenv("WINDOW_END_KST", "").strip()
    us = os.getenv("WINDOW_START_UTC", "").strip()
    ue = os.getenv("WINDOW_END_UTC", "").strip()

    if ks and ke:
        try:
            start_kst = datetime.strptime(ks, fmt).replace(tzinfo=KST)
            end_kst   = datetime.strptime(ke, fmt).replace(tzinfo=KST)
            return start_kst.astimezone(UTC), end_kst.astimezone(UTC), "KST"
        except Exception as e:
            _modlog.warning(f"Failed to parse KST window: {e}")

    if us and ue:
        try:
            start_utc = datetime.strptime(us, fmt).replace(tzinfo=UTC)
            end_utc   = datetime.strptime(ue, fmt).replace(tzinfo=UTC)
            return start_utc, end_utc, "UTC"
        except Exception as e:
            _modlog.warning(f"Failed to parse UTC window: {e}")

    return None, None, ""

def within_window(ts_utc: datetime, start_utc: Optional[datetime], end_utc: Optional[datetime]) -> bool:
    if start_utc and ts_utc < start_utc:
        return False
    if end_utc and ts_utc > end_utc:
        return False
    return True

# -------------------- ë©”ì¸ í´ë˜ìŠ¤ --------------------
class EtherscanAnalyzer:
    def __init__(self, api_key: str, config: Config = None, logger: Optional[logging.Logger] = None):
        self.api_key = api_key or os.getenv("ETHERSCAN_API_KEY")
        if not self.api_key or self.api_key == "YOUR_API_KEY":
            raise ValueError("ETHERSCAN_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        self.config = config or Config()
        self.logger = logger or logging.getLogger(__name__)
        self.base_url = "https://api.etherscan.io/api"         # v1
        self.base_url_v2 = "https://api.etherscan.io/v2/api"   # v2

        # ì„¸ì…˜ + Retry
        self.session = requests.Session()
        retries = Retry(
            total=5,
            backoff_factor=0.6,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=frozenset(["GET"])
        )
        self.session.mount("https://", HTTPAdapter(max_retries=retries))

        # ì‹œê°„ ìœˆë„ìš°(UTC) ì„¤ì •
        self.window_start_utc, self.window_end_utc, self.window_kind = parse_window_from_env()
        if self.window_start_utc or self.window_end_utc:
            s = self.window_start_utc.astimezone(KST).strftime("%Y-%m-%d %H:%M:%S %Z") if self.window_start_utc else "-"
            e = self.window_end_utc.astimezone(KST).strftime("%Y-%m-%d %H:%M:%S %Z") if self.window_end_utc else "-"
            self.logger.info(f"Applied time window [{self.window_kind}]: {s} ~ {e}")

        # ë¼ìš°í„° ì£¼ì†Œë“¤
        self.routers: Set[str] = {
            "0x7a250d5630b4cf539739df2c5dacb4c659f2488d",  # Uniswap V2
            "0xe592427a0aece92de3edee1f18e0157c05861564",  # Uniswap V3
            "0x68b3465833fb72a70ecdf485e0e4c7bd8665fc45",  # Uniswap V3 Router 2
            "0x1111111254fb6c44bac0bed2854e76f90643097d",  # 1inch v4
            "0x1111111254eeb25477b68fb85ed929f73a960582",  # 1inch v5
            "0xdef1c0ded9bec7f1a1670819833240f027b25eff",  # 0x
            "0x881d40237659c251811cec9c364ef91dc08d300c",  # MetaMask swap
        }
        suspect_router = os.getenv("EXTRA_ROUTER", "").strip().lower()
        if suspect_router.startswith("0x") and len(suspect_router) == 42:
            self.routers.add(suspect_router)
            self.logger.info(f"Added EXTRA_ROUTER: {suspect_router}")

        # í•¨ìˆ˜ ì‹œê·¸ë‹ˆì²˜ ë§¤í•‘(í™•ì¥)
        self.sig_map: Dict[str, str] = {
            # Uniswap V2
            "0x18cbafe5": "swapExactTokensForETH",
            "0x791ac947": "swapExactTokensForETHSupportingFeeOnTransferTokens",
            "0x4a25d94a": "swapTokensForExactETH",
            "0x7ff36ab5": "swapExactETHForTokens",
            "0xfb3bdb41": "swapETHForExactTokens",
            "0x38ed1739": "swapExactTokensForTokens",
            "0x8803dbee": "swapTokensForExactTokens",
            "0xb6f9de95": "swapExactETHForTokensSupportingFeeOnTransferTokens",
            # Uniswap V3 (ê°„ë‹¨ í‘œê¸° â€” ì‹¤ì œ ë°©í–¥ì„±ì€ path í•„ìš”)
            "0x414bf389": "exactInputSingle",   # 0x414bf389 ì˜›/ìƒˆ ë²„ì „ ì°¨ì´ ìˆìŒ
            "0xb858183f": "exactInput",
            # 1inch/0x ë“±
            "0x12aa3caf": "1inch.swap",
            "0x0502b1c5": "1inch.unoswap",
            "0x2e95b6c8": "1inch.unoswap",
            # ERC-20
            "0xa9059cbb": "erc20.transfer",
            "0x095ea7b3": "erc20.approve",
        }

        # ì‹¤í–‰ ì¤‘ í† í° ì£¼ì†Œ(ë ˆí¬íŠ¸ í‘œê¸°ë¥¼ ìœ„í•´ mainì—ì„œ ì£¼ì…)
        self.token_address: Optional[str] = None

    # -------------------- í™€ë” ìˆ˜ì§‘ --------------------
    def get_top_holders_v2(self, token_addr: str, limit: int) -> List[str]:
        params = {
            "chainid": 1,
            "module": "token",
            "action": "topholders",
            "contractaddress": token_addr,
            "offset": min(limit, 1000),
            "apikey": self.api_key,
        }
        try:
            r = self.session.get(self.base_url_v2, params=params, timeout=self.config.REQUEST_TIMEOUT)
            r.raise_for_status()
            data = r.json()
            result = data.get("result", {})
            holders = result.get("holders", []) if isinstance(result, dict) else result
            if isinstance(holders, list) and holders:
                return [h.get("address", "").lower() for h in holders if h.get("address")][:limit]
        except Exception as e:
            self.logger.warning(f"v2 topholders API failed: {e}")
        return []

    def get_holders_from_tokentx_v1(self, token_addr: str, limit: int) -> List[str]:
        page = 1
        offset = min(10000, self.config.BATCH_SIZE)
        max_pages = min(20, 10000 // max(1, offset))  # ì„±ëŠ¥ ì œí•œ
        addrs: Set[str] = set()
        zero = "0x0000000000000000000000000000000000000000"
        token_addr_l = token_addr.lower()
        excluded = {zero, token_addr_l} | self.routers

        def keep(addr: str) -> bool:
            return (addr and addr.startswith("0x") and len(addr) == 42 and addr not in excluded)

        while page <= max_pages and len(addrs) < limit:
            params = {
                "module": "account",
                "action": "tokentx",
                "contractaddress": token_addr,
                "page": page,
                "offset": offset,
                "sort": "desc",
                "apikey": self.api_key
            }
            try:
                r = self.session.get(self.base_url, params=params, timeout=self.config.REQUEST_TIMEOUT)
                r.raise_for_status()
                data = r.json()
                if data.get("status") == "0" and "No transactions found" in data.get("message", ""):
                    break
                rows = data.get("result", []) or []
                if not rows:
                    break

                new_addrs = {
                    addr for t in rows
                    for addr in [(t.get("from") or "").lower(), (t.get("to") or "").lower()]
                    if keep(addr) and addr not in addrs
                }
                addrs.update(new_addrs)

                if len(rows) < offset:
                    break
                page += 1

                # ë ˆì´íŠ¸ë¦¬ë°‹ ì§€í„°
                time.sleep(self.config.REQUEST_DELAY * random.uniform(0.9, 1.3))

                if len(addrs) >= limit:
                    break

            except Exception as e:
                self.logger.warning(f"Error fetching tokentx page {page}: {e}")
                break

        res = list(addrs)[:limit]
        self.logger.info(f"Collected {len(res)} holder candidates from tokentx (v1)")
        return res

    def get_holders(self, token_addr: str, limit: int) -> List[str]:
        addrs = []
        try:
            addrs = self.get_top_holders_v2(token_addr, limit)
            if len(addrs) >= min(limit // 2, 100):
                self.logger.info(f"Fetched {len(addrs)} top holders via v2")
                if len(addrs) < limit:
                    self.logger.info(f"v2ì—ì„œ {len(addrs)}ê°œë§Œ ìˆ˜ì§‘, v1ìœ¼ë¡œ ì¶”ê°€ ìˆ˜ì§‘ ì‹œë„")
                    additional = self.get_holders_from_tokentx_v1(token_addr, limit - len(addrs))
                    addrs.extend([a for a in additional if a not in addrs])
                return addrs[:limit]
            self.logger.warning("v2 topholders insufficient, falling back to v1 tokentx")
        except Exception as e:
            self.logger.warning(f"v2 topholders failed ({e}), falling back to v1 tokentx")
        return self.get_holders_from_tokentx_v1(token_addr, limit)

    # -------------------- íŠ¸ëœì­ì…˜ ìˆ˜ì§‘/ì •ê·œí™” --------------------
    def _validate_api_response(self, data: Dict) -> bool:
        if not isinstance(data, dict):
            return False
        if data.get("message") == "NOTOK":
            result = str(data.get("result", ""))
            if "rate limit" in result.lower():
                self.logger.warning("Rate limit hit, waiting...")
                time.sleep(2.0)
                return False
            if "no transactions found" in result.lower():
                return True
            self.logger.warning(f"API returned NOTOK: {result}")
            return False
        return True

    def get_txlist(self, address: str, startblock: int = 0, endblock: int = 99999999, sort: str = "asc") -> List[Dict]:
        self.logger.info(f"Fetching transactions for address: {address}")
        page, offset = 1, self.config.BATCH_SIZE
        max_pages = min(100, 10000 // max(1, offset))
        all_rows: List[Dict] = []

        while True:
            if page > max_pages:
                self.logger.warning(f"Reached Etherscan window limit ({max_pages} pages) for {address}")
                break

            params = {
                "module": "account",
                "action": "txlist",
                "address": address,
                "startblock": startblock,
                "endblock": endblock,
                "page": page,
                "offset": offset,
                "sort": sort,
                "apikey": self.api_key
            }

            for attempt in range(self.config.MAX_RETRIES):
                try:
                    response = self.session.get(self.base_url, params=params, timeout=self.config.REQUEST_TIMEOUT)
                    response.raise_for_status()
                    data = response.json()
                    if not self._validate_api_response(data):
                        if attempt == self.config.MAX_RETRIES - 1:
                            self.logger.error(f"Failed after {self.config.MAX_RETRIES} attempts for {address}")
                            return all_rows
                        time.sleep(2 ** attempt)
                        continue

                    if data.get("status") == "0" and "No transactions found" in data.get("message", ""):
                        self.logger.info(f"No transactions found for {address}")
                        return all_rows

                    batch = data.get("result", []) or []
                    if not isinstance(batch, list) or not batch:
                        return all_rows

                    all_rows.extend(batch)
                    self.logger.info(f"Fetched {len(batch)} transactions (page {page}) for {address}")

                    if len(batch) < offset:
                        return all_rows

                    page += 1
                    time.sleep(max(self.config.REQUEST_DELAY, 0.25) * random.uniform(0.9, 1.3))
                    break

                except requests.RequestException as e:
                    self.logger.warning(f"Request attempt {attempt + 1} failed for {address}: {e}")
                    if attempt == self.config.MAX_RETRIES - 1:
                        self.logger.error(f"All retry attempts failed for {address}")
                        return all_rows
                    time.sleep(2 ** attempt)
            else:
                break

        self.logger.info(f"Total {len(all_rows)} transactions fetched for {address}")
        return all_rows

    def _is_sell(self, method_name: str) -> bool:
        """ë§¤ë„ ê±°ë˜ ì¶”ì • (ë³´ìˆ˜ì ìœ¼ë¡œ í™•ëŒ€)"""
        if not method_name:
            return False
        n = method_name.lower()
        # ì •í™•í•œ ë°©í–¥ì„±ì€ ë¡œê·¸/ê²½ë¡œ í•„ìš”. ì—¬ê¸°ì„œëŠ” recallì„ ë†’ì´ëŠ” íƒ€í˜‘ì•ˆ.
        return any(x in n for x in [
            "tokensforeth",     # v2 íŒ¨í„´
            "exactinputsingle", # v3 ì¶”ì •
            "exactinput"        # v3 ì¶”ì •
        ])

    def normalize_tx_rows(self, rows: List[Dict], holder: str) -> pd.DataFrame:
        if not rows:
            return pd.DataFrame()

        processed_rows = []
        for tx in rows:
            try:
                if not all(k in tx for k in ["hash", "timeStamp", "blockNumber", "to"]):
                    continue

                to_addr = (tx.get("to") or "").lower()
                if to_addr not in self.routers:
                    continue  # ë¼ìš°í„° í˜¸ì¶œë§Œ ë¶„ì„

                ts = int(tx["timeStamp"])
                time_utc = datetime.fromtimestamp(ts, UTC)
                if not within_window(time_utc, self.window_start_utc, self.window_end_utc):
                    continue

                input_data = tx.get("input", "") or ""
                method_sig = input_data[:10].lower() if (input_data.startswith("0x") and len(input_data) >= 10) else ""
                method_name = self.sig_map.get(method_sig, f"sig:{method_sig}" if method_sig else "unknown")

                is_error = str(tx.get("isError", "0")) == "1"
                receipt_failed = str(tx.get("txreceipt_status", "1")) == "0"
                status = "fail" if (is_error or receipt_failed) else "success"

                block_num = int(tx["blockNumber"])
                gas_used = int(tx.get("gasUsed", 0))
                gas_price = int(tx.get("gasPrice", 0))
                value_eth = float(tx.get("value", 0)) / 1e18
                gas_fee_eth = (gas_used * gas_price) / 1e18 if gas_used and gas_price else 0.0

                processed_rows.append({
                    "holder": holder.lower(),
                    "hash": tx["hash"],
                    "time_utc": time_utc,
                    "block": block_num,
                    "to": to_addr,
                    "method_guess": method_name,
                    "is_sell_path": self._is_sell(method_name),
                    "value_eth": value_eth,
                    "gas_used": gas_used,
                    "gas_price_gwei": gas_price / 1e9 if gas_price else 0.0,
                    "gas_fee_eth": gas_fee_eth,
                    "isError": tx.get("isError", "0"),
                    "txreceipt_status": tx.get("txreceipt_status", "1"),
                    "status": status,
                })
            except Exception as e:
                self.logger.warning(f"Error processing tx {tx.get('hash', '')}: {e}")
                continue

        df = pd.DataFrame(processed_rows)
        if not df.empty:
            df = df.drop_duplicates("hash", keep="first")
        self.logger.info(f"Processed {len(df)} router transactions in window for {holder}")
        return df

    # -------------------- ë¶„ì„ --------------------
    def _enrich_gas_with_receipts(self, df: pd.DataFrame) -> None:
        if df.empty:
            return
        target = df[df["status"] == "fail"].sort_values("time_utc", ascending=False).head(self.config.ENRICH_RECEIPT_TOPN)
        for h in target["hash"].tolist():
            try:
                r = self.session.get(self.base_url, params={
                    "module": "proxy",
                    "action": "eth_getTransactionReceipt",
                    "txhash": h,
                    "apikey": self.api_key
                }, timeout=self.config.REQUEST_TIMEOUT)
                js = r.json()
                result = js.get("result") or {}
                eff = result.get("effectiveGasPrice")
                gas_used_hex = result.get("gasUsed")
                if eff and gas_used_hex:
                    eff_int = int(eff, 16)
                    used = int(gas_used_hex, 16)
                    df.loc[df["hash"] == h, "gas_price_gwei"] = eff_int / 1e9
                    df.loc[df["hash"] == h, "gas_fee_eth"] = (eff_int * used) / 1e18
                time.sleep(0.1)
            except Exception:
                pass

    def cluster_failures(self, df: pd.DataFrame, window_minutes: Optional[int] = None) -> pd.DataFrame:
        if df.empty:
            return pd.DataFrame(columns=["holder", "start", "end", "count", "gas_waste_eth"])
        window_minutes = window_minutes or self.config.CLUSTER_WINDOW_MINUTES
        fails = df[df["status"] == "fail"].sort_values("time_utc").copy()
        if fails.empty:
            return pd.DataFrame(columns=["holder", "start", "end", "count", "gas_waste_eth"])

        clusters = []
        i = 0
        while i < len(fails):
            start_time = fails.iloc[i]["time_utc"]
            start_idx = i
            while i + 1 < len(fails):
                next_time = fails.iloc[i + 1]["time_utc"]
                if (next_time - start_time).total_seconds() <= window_minutes * 60:
                    i += 1
                else:
                    break
            end_idx = i
            count = end_idx - start_idx + 1
            if count >= self.config.MIN_CLUSTER_SIZE:
                cluster_txs = fails.iloc[start_idx:end_idx + 1]
                gas_waste = cluster_txs["gas_fee_eth"].sum()
                clusters.append({
                    "holder": fails.iloc[start_idx]["holder"],
                    "start": start_time,
                    "end": fails.iloc[end_idx]["time_utc"],
                    "count": count,
                    "gas_waste_eth": gas_waste
                })
            i += 1
        return pd.DataFrame(clusters)

    def detect_honeypot_indicators(self, summary_df: pd.DataFrame) -> Dict[str, List[str]]:
        indicators = {}
        for _, row in summary_df.iterrows():
            holder = row['holder']
            holder_indicators = []

            if row['fail_rate'] >= self.config.HIGH_FAIL_RATE_THRESHOLD:
                holder_indicators.append(f"ë†’ì€ ì‹¤íŒ¨ìœ¨: {row['fail_rate']:.1%}")
            if row['max_consecutive_fails'] >= self.config.CONSECUTIVE_FAIL_THRESHOLD:
                holder_indicators.append(f"ì—°ì† ì‹¤íŒ¨: {row['max_consecutive_fails']}íšŒ")
            if row['clusters_2h_count'] >= 3:
                holder_indicators.append(f"ì‹¤íŒ¨ í´ëŸ¬ìŠ¤í„°: {row['clusters_2h_count']}ê°œ")
            if row['total_gas_waste_eth'] > 0.05:
                holder_indicators.append(f"ê°€ìŠ¤ ë‚­ë¹„: {row['total_gas_waste_eth']:.3f} ETH")
            if 'sell_fail_rate' in row and row['sell_txs'] >= 3 and row['sell_fail_rate'] >= 0.7:
                holder_indicators.append(f"ë§¤ë„ ì‹¤íŒ¨ìœ¨ ë†’ìŒ: {row['sell_fail_rate']:.1%} / {row['sell_txs']}ê±´")

            if holder_indicators:
                indicators[holder] = holder_indicators
        return indicators

    def analyze_holder(self, address: str) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        self.logger.info(f"Starting analysis for holder: {address}")
        raw_txs = self.get_txlist(address)
        df = self.normalize_tx_rows(raw_txs, address)

        if df.empty:
            empty_summary = pd.DataFrame([{
                "holder": address.lower(),
                "tx_to_router": 0,
                "fails": 0,
                "success": 0,
                "fail_rate": 0.0,
                "max_consecutive_fails": 0,
                "avg_gas_failed": 0.0,
                "total_gas_waste_eth": 0.0,
                "clusters_2h_count": 0,
                "sell_txs": 0,
                "sell_fail_rate": 0.0,
                "first_tx": None,
                "last_tx": None
            }])
            return df, empty_summary, pd.DataFrame()

        if self.config.USE_RECEIPT_GAS:
            self._enrich_gas_with_receipts(df)

        df = df.sort_values("time_utc").reset_index(drop=True)

        total_txs = len(df)
        failed_txs = int((df["status"] == "fail").sum())
        success_txs = total_txs - failed_txs
        fail_rate = failed_txs / total_txs if total_txs else 0.0

        # ì—°ì† ì‹¤íŒ¨ ê³„ì‚°
        consecutive_fails = 0
        max_consecutive_fails = 0
        for st in df["status"]:
            if st == "fail":
                consecutive_fails += 1
                max_consecutive_fails = max(max_consecutive_fails, consecutive_fails)
            else:
                consecutive_fails = 0

        # ê°€ìŠ¤ë¹„ ë¶„ì„
        failed_df = df[df["status"] == "fail"]
        avg_gas_failed = float(failed_df["gas_used"].mean()) if not failed_df.empty else 0.0
        total_gas_waste = float(failed_df["gas_fee_eth"].sum()) if not failed_df.empty else 0.0

        # ë§¤ë„ ê±°ë˜ ë¶„ì„
        sell_df = df[df["is_sell_path"] == True]
        sell_fail_rate = float((sell_df["status"] == "fail").mean()) if not sell_df.empty else 0.0

        # ì‹¤íŒ¨ í´ëŸ¬ìŠ¤í„° ë¶„ì„
        clusters = self.cluster_failures(df)

        summary = pd.DataFrame([{
            "holder": address.lower(),
            "tx_to_router": total_txs,
            "fails": failed_txs,
            "success": success_txs,
            "fail_rate": round(fail_rate, 4),
            "max_consecutive_fails": int(max_consecutive_fails),
            "avg_gas_failed": round(avg_gas_failed, 0),
            "total_gas_waste_eth": round(total_gas_waste, 6),
            "clusters_2h_count": len(clusters),
            "sell_txs": int(len(sell_df)),
            "sell_fail_rate": round(sell_fail_rate, 4),
            "first_tx": df["time_utc"].min(),
            "last_tx": df["time_utc"].max()
        }])

        self.logger.info(f"Analysis complete for {address}: {total_txs} txs (window), {fail_rate:.1%} fail rate")
        return df, summary, clusters

    def safe_concat_dataframes(self, df_list: List[pd.DataFrame], name: str) -> pd.DataFrame:
        if not df_list:
            self.logger.warning(f"No dataframes to concat for {name}")
            return pd.DataFrame()
        try:
            result = pd.concat(df_list, ignore_index=True)
            self.logger.info(f"Successfully concatenated {len(df_list)} dataframes for {name}")
            return result
        except Exception as e:
            self.logger.error(f"Error concatenating dataframes for {name}: {e}")
            return pd.DataFrame()

    def analyze_multiple_holders(self, holders: List[str]) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        total_holders = len(holders)
        self.logger.info(f"Starting analysis for {total_holders} token holders")

        all_transactions, all_summaries, all_clusters = [], [], []
        failed_holders = []

        for i, holder in enumerate(holders, 1):
            try:
                tx_df, summary_df, cluster_df = self.analyze_holder(holder)
                if not tx_df.empty:
                    all_transactions.append(tx_df)
                if not summary_df.empty:
                    all_summaries.append(summary_df)
                if not cluster_df.empty:
                    all_clusters.append(cluster_df)

                if i % 50 == 0 or i == total_holders:
                    progress = (i / total_holders) * 100
                    self.logger.info(f"Progress: {i}/{total_holders} ({progress:.1f}%) completed")
                    print(f"ì§„í–‰ë¥ : {progress:.1f}% ({i}/{total_holders})")
            except Exception as e:
                failed_holders.append(holder)
                self.logger.error(f"Error analyzing holder {holder}: {e}")
                continue

        if failed_holders:
            self.logger.warning(f"Failed to analyze {len(failed_holders)} holders")

        combined_txs = self.safe_concat_dataframes(all_transactions, "transactions")
        combined_summaries = self.safe_concat_dataframes(all_summaries, "summaries")
        combined_clusters = self.safe_concat_dataframes(all_clusters, "clusters")
        return combined_txs, combined_summaries, combined_clusters

    # -------------------- ì¶œë ¥/ì €ì¥ --------------------
    def generate_report(self, tx_df: pd.DataFrame, summary_df: pd.DataFrame, cluster_df: pd.DataFrame, token_name: str) -> str:
        report = []
        report.append("=" * 80)
        report.append(f"{token_name.upper()} í† í° í—ˆë‹ˆíŒŸ ë¶„ì„ ë³´ê³ ì„œ")
        if hasattr(self, 'token_address') and self.token_address:
            report.append(f"í† í° ì£¼ì†Œ: {self.token_address}")
        report.append("=" * 80)

        report.append(f"ë¶„ì„ ì‹œê°„: {datetime.now(UTC).astimezone(KST).strftime('%Y-%m-%d %H:%M:%S %Z')}")
        if self.window_start_utc or self.window_end_utc:
            s = self.window_start_utc.astimezone(KST).strftime('%Y-%m-%d %H:%M:%S %Z') if self.window_start_utc else "-"
            e = self.window_end_utc.astimezone(KST).strftime('%Y-%m-%d %H:%M:%S %Z') if self.window_end_utc else "-"
            report.append(f"ì ìš© ìœˆë„ìš°: {s} ~ {e}")
        else:
            report.append("ì ìš© ìœˆë„ìš°: ì „ì²´ ê¸°ê°„")
        report.append("")

        if summary_df.empty:
            report.append("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return "\n".join(report)

        total_holders = len(summary_df)
        holders_with_txs = len(summary_df[summary_df["tx_to_router"] > 0])
        total_txs = int(summary_df["tx_to_router"].sum())
        total_fails = int(summary_df["fails"].sum())
        overall_fail_rate = (total_fails / total_txs) if total_txs > 0 else 0.0
        total_gas_waste = float(summary_df["total_gas_waste_eth"].sum())

        report.append("ğŸ“Š ì „ì²´ í†µê³„:")
        report.append(f"  - ë¶„ì„ í™€ë” ìˆ˜: {total_holders}")
        report.append(f"  - ê±°ë˜ í™œë™ í™€ë”: {holders_with_txs}")
        report.append(f"  - ì´ ë¼ìš°í„° ê±°ë˜(ìœˆë„ìš° ë‚´): {total_txs:,}")
        report.append(f"  - ì‹¤íŒ¨ ê±°ë˜: {total_fails:,}")
        report.append(f"  - ì „ì²´ ì‹¤íŒ¨ìœ¨: {overall_fail_rate:.1%}")
        report.append(f"  - ì´ ê°€ìŠ¤ ë‚­ë¹„: {total_gas_waste:.4f} ETH")
        report.append("")

        indicators = self.detect_honeypot_indicators(summary_df)
        high_risk_holders = len([h for h, inds in indicators.items() if len(inds) >= 2])

        if indicators:
            report.append("ğŸš¨ í—ˆë‹ˆíŒŸ ì§€í‘œ íƒì§€:")
            report.append(f"  - ì§€í‘œ ë³´ìœ  í™€ë”: {len(indicators)}ëª…")
            report.append(f"  - ê³ ìœ„í—˜ í™€ë” (2ê°œ ì´ìƒ ì§€í‘œ): {high_risk_holders}ëª…")
            report.append("")
            sorted_indicators = sorted(indicators.items(), key=lambda x: len(x[1]), reverse=True)
            for i, (holder, holder_indicators) in enumerate(sorted_indicators[:10], 1):
                report.append(f"  [{i}] {holder[:10]}...:")
                for indicator in holder_indicators:
                    report.append(f"    âš ï¸  {indicator}")
            if len(indicators) > 10:
                report.append(f"  ... ë° {len(indicators) - 10}ëª… ì¶”ê°€")
            report.append("")

        # ë§¤ë„ ì‹¤íŒ¨ ë¶„ì„(ìš”ì•½)
        high_sell_fail = pd.DataFrame()
        sell_analysis = summary_df[summary_df["sell_txs"] >= 3]
        if not sell_analysis.empty:
            high_sell_fail = sell_analysis[sell_analysis["sell_fail_rate"] >= 0.7]
            report.append("ğŸ”´ ë§¤ë„ ì‹¤íŒ¨ ë¶„ì„:")
            report.append(f"  - ë§¤ë„ ì‹œë„ í™€ë” (3íšŒ ì´ìƒ): {len(sell_analysis)}ëª…")
            report.append(f"  - ë§¤ë„ ì‹¤íŒ¨ìœ¨ 70% ì´ìƒ: {len(high_sell_fail)}ëª…")
            if not high_sell_fail.empty:
                avg_sell_fail_rate = high_sell_fail["sell_fail_rate"].mean()
                report.append(f"  - í‰ê·  ë§¤ë„ ì‹¤íŒ¨ìœ¨: {avg_sell_fail_rate:.1%}")
            report.append("")

        # í´ëŸ¬ìŠ¤í„° ë¶„ì„
        if not cluster_df.empty:
            total_clusters = len(cluster_df)
            avg_cluster_size = cluster_df["count"].mean()
            total_cluster_waste = cluster_df["gas_waste_eth"].sum()
            report.append("ğŸ“ˆ ì‹¤íŒ¨ í´ëŸ¬ìŠ¤í„° ë¶„ì„:")
            report.append(f"  - ì´ í´ëŸ¬ìŠ¤í„°: {total_clusters}ê°œ")
            report.append(f"  - í‰ê·  í´ëŸ¬ìŠ¤í„° í¬ê¸°: {avg_cluster_size:.1f}íšŒ")
            report.append(f"  - í´ëŸ¬ìŠ¤í„° ê°€ìŠ¤ ë‚­ë¹„: {total_cluster_waste:.4f} ETH")
            report.append("")

        # í™€ë”ë³„ ë¶„ì„(ë§¤ë„ ì‹¤íŒ¨ì ìš”ì•½)
        report.append("ğŸ‘¥ í™€ë”ë³„ ë¶„ì„ ê²°ê³¼:")
        sell_fail_holders = summary_df[(summary_df["sell_txs"] >= 1) & (summary_df["sell_fail_rate"] > 0)]\
            .sort_values(["sell_fail_rate", "total_gas_waste_eth"], ascending=False)
        if not sell_fail_holders.empty:
            report.append(f"  ğŸ“Š ë§¤ë„ ì‹œë„ ì‹¤íŒ¨ í™€ë”: {len(sell_fail_holders)}ëª…")
            report.append("")
            for idx, (_, row) in enumerate(sell_fail_holders.head(50).iterrows(), 1):
                holder_short = row['holder'][:12] + "..."
                report.append(f"  [{idx}] {holder_short}:")
                report.append(
                    f"    ì „ì²´ê±°ë˜: {int(row['tx_to_router'])} | ì „ì²´ì‹¤íŒ¨ìœ¨: {row['fail_rate']:.1%} | "
                    f"ë§¤ë„ì‹¤íŒ¨ìœ¨: {row['sell_fail_rate']:.1%} ({int(row['sell_txs'])}ê±´ ì¤‘)"
                )
                report.append(
                    f"    ì—°ì†ì‹¤íŒ¨: {int(row['max_consecutive_fails'])}íšŒ | "
                    f"ê°€ìŠ¤ë‚­ë¹„: {row['total_gas_waste_eth']:.4f} ETH | í´ëŸ¬ìŠ¤í„°: {int(row['clusters_2h_count'])}ê°œ"
                )
        else:
            report.append("  ë§¤ë„ ì‹œë„ ì‹¤íŒ¨ í™€ë”ê°€ ì—†ìŠµë‹ˆë‹¤.")
        report.append("")

        # ê²°ë¡ (ê°„ë‹¨ íŒì •)
        report.append(f"ğŸ“‹ {token_name.upper()} í† í° ë¶„ì„ ê²°ë¡ :")
        if overall_fail_rate >= 0.20:
            report.append("  ğŸ”´ HIGH RISK: ë†’ì€ ì „ì²´ ì‹¤íŒ¨ìœ¨")
        elif overall_fail_rate >= 0.10:
            report.append("  ğŸŸ¡ MEDIUM RISK: ì¤‘ê°„ ìˆ˜ì¤€ ì‹¤íŒ¨ìœ¨")
        else:
            report.append("  ğŸŸ¢ LOW RISK: ë‚®ì€ ì‹¤íŒ¨ìœ¨")

        if high_risk_holders >= 50:
            report.append("  ğŸ”´ HIGH RISK: ë§¤ìš° ë‹¤ìˆ˜ í™€ë”ì—ì„œ í—ˆë‹ˆíŒŸ ì§€í‘œ ë°œê²¬")
        elif high_risk_holders >= 20:
            report.append("  ğŸŸ¡ MEDIUM RISK: ë‹¤ìˆ˜ í™€ë”ì—ì„œ í—ˆë‹ˆíŒŸ ì§€í‘œ ë°œê²¬")
        elif high_risk_holders >= 5:
            report.append("  ğŸŸ¡ MEDIUM RISK: ì¼ë¶€ í™€ë”ì—ì„œ í—ˆë‹ˆíŒŸ ì§€í‘œ ë°œê²¬")
        else:
            report.append("  ğŸŸ¢ LOW RISK: í—ˆë‹ˆíŒŸ ì§€í‘œ ë³´ìœ  í™€ë” ì ìŒ")

        num_high_sell_fail = len(high_sell_fail) if not high_sell_fail.empty else 0
        if not sell_analysis.empty and num_high_sell_fail >= 10:
            report.append("  ğŸ”´ HIGH RISK: ë‹¤ìˆ˜ í™€ë”ì˜ ë§¤ë„ ì‹œë„ ì‹¤íŒ¨")
        elif not sell_analysis.empty and num_high_sell_fail >= 3:
            report.append("  ğŸŸ¡ MEDIUM RISK: ì¼ë¶€ í™€ë”ì˜ ë§¤ë„ ì‹œë„ ì‹¤íŒ¨")

        risk_factors = 0
        if overall_fail_rate >= 0.10: risk_factors += 1
        if high_risk_holders >= 20: risk_factors += 1
        if num_high_sell_fail >= 3: risk_factors += 1
        if total_gas_waste >= 50.0: risk_factors += 1
        if len(indicators) >= 50: risk_factors += 1

        report.append("")
        if risk_factors >= 4:
            report.append(f"  ğŸš¨ ìµœì¢… ê²°ë¡ : {token_name.upper()}ëŠ” í—ˆë‹ˆíŒŸ ìŠ¤ìº  í† í°ìœ¼ë¡œ ê°•ë ¥íˆ ì˜ì‹¬ë¨")
        elif risk_factors >= 3:
            report.append(f"  ğŸ”´ ìµœì¢… ê²°ë¡ : {token_name.upper()}ëŠ” í—ˆë‹ˆíŒŸ ê°€ëŠ¥ì„±ì´ ë§¤ìš° ë†’ì€ ìœ„í—˜í•œ í† í°")
        elif risk_factors >= 2:
            report.append(f"  ğŸŸ¡ ìµœì¢… ê²°ë¡ : {token_name.upper()}ëŠ” í—ˆë‹ˆíŒŸ ê°€ëŠ¥ì„±ì´ ìˆëŠ” ìœ„í—˜í•œ í† í°")
        else:
            report.append("  ğŸŸ¢ ìµœì¢… ê²°ë¡ : ì¼ë¶€ ì§€í‘œ ì¡´ì¬í•˜ë‚˜ ê²°ì •ì  ì¦ê±° ë¶€ì¡±")

        report.append("")
        report.append(f"  ğŸ“Š ìœ„í—˜ ìš”ì†Œ ì ìˆ˜: {risk_factors}/5")
        report.append(f"  ğŸ“Š ì§€í‘œ ë³´ìœ  í™€ë” ë¹„ìœ¨: {len(indicators)}/{total_holders} ({len(indicators)/total_holders*100:.1f}%)")
        report.append(f"  ğŸ“Š ê³ ìœ„í—˜ í™€ë” ë¹„ìœ¨: {high_risk_holders}/{total_holders} ({(high_risk_holders/total_holders*100 if total_holders else 0):.1f}%)")

        report.append("=" * 80)
        return "\n".join(report)

    def save_results(self, tx_df: pd.DataFrame, summary_df: pd.DataFrame, cluster_df: pd.DataFrame, token_name: str) -> Dict[str, str]:
        timestamp = datetime.now(UTC).strftime("%Y%m%d_%H%M%S")
        holder_count = len(summary_df) if not summary_df.empty else 0
        token_lower = token_name.lower()

        filenames = {
            "transactions": f"{token_lower}_transactions_{holder_count}_{timestamp}.csv",
            "summary": f"{token_lower}_summary_{holder_count}_{timestamp}.csv",
            "clusters": f"{token_lower}_clusters_{holder_count}_{timestamp}.csv",
            "report": f"{token_lower}_report_{holder_count}_{timestamp}.txt"
        }
        saved = {}
        try:
            if not tx_df.empty:
                tx_df.to_csv(filenames["transactions"], index=False, encoding='utf-8-sig')
                self.logger.info(f"Saved transactions to {filenames['transactions']}")
                saved["transactions"] = filenames["transactions"]

            if not summary_df.empty:
                summary_df.to_csv(filenames["summary"], index=False, encoding='utf-8-sig')
                self.logger.info(f"Saved summary to {filenames['summary']}")
                saved["summary"] = filenames["summary"]

            if not cluster_df.empty:
                cluster_df.to_csv(filenames["clusters"], index=False, encoding='utf-8-sig')
                self.logger.info(f"Saved clusters to {filenames['clusters']}")
                saved["clusters"] = filenames["clusters"]

            report = self.generate_report(tx_df, summary_df, cluster_df, token_name)
            with open(filenames["report"], 'w', encoding='utf-8') as f:
                f.write(report)
            self.logger.info(f"Saved report to {filenames['report']}")
            saved["report"] = filenames["report"]
            return saved

        except Exception as e:
            self.logger.error(f"Error saving results: {e}")
            return saved

# -------------------- ë©”ì¸ --------------------
def main():
    # === í™˜ê²½ë³€ìˆ˜ í•„ìˆ˜ ===
    api_key = os.getenv("ETHERSCAN_API_KEY", "").strip()
    token_address = os.getenv("TOKEN_ADDRESS", "").strip().lower()
    token_name = os.getenv("TOKEN_NAME", "").strip()
    holder_limit_env = os.getenv("HOLDER_LIMIT", "").strip()

    missing = []
    if not api_key: missing.append("ETHERSCAN_API_KEY")
    if not token_address: missing.append("TOKEN_ADDRESS")
    if not token_name: missing.append("TOKEN_NAME")
    if not holder_limit_env: missing.append("HOLDER_LIMIT")
    if missing:
        print("\nâŒ í•„ìˆ˜ í™˜ê²½ë³€ìˆ˜ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤:")
        for m in missing:
            print(f"  - {m}")
        print("\nPowerShell ì˜ˆì‹œ:")
        print('$env:ETHERSCAN_API_KEY = "ì‹¤ì œ_API_í‚¤"')
        print('$env:TOKEN_ADDRESS = "í† í°_ì£¼ì†Œ"')
        print('$env:TOKEN_NAME = "í† í°ì´ë¦„"')
        print('$env:HOLDER_LIMIT = "í™€ë”ìˆ˜"')
        sys.exit(1)

    try:
        holder_limit = int(holder_limit_env)
    except ValueError:
        print("âŒ HOLDER_LIMIT ëŠ” ì •ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤. ì˜ˆ: 335")
        sys.exit(1)

    logger = setup_logging(token_name)

    config = Config()
    config.HOLDER_LIMIT = holder_limit
    # ì„±ëŠ¥/ì •í™•ë„ íŠ¸ë ˆì´ë“œì˜¤í”„ ì¡°ì • ê°€ëŠ¥í•˜ë©´ ì—¬ê¸°ì„œ í™˜ê²½ë³€ìˆ˜ë¡œ ì¶”ê°€ ì„¸íŒ…í•´ë„ ë¨.

    logger.info(f"=== {token_name} í† í° ë¶„ì„ ì„¤ì • ===")
    logger.info(f"í† í° ì´ë¦„: {token_name}")
    logger.info(f"í† í° ì£¼ì†Œ: {token_address}")
    logger.info(f"ëª©í‘œ í™€ë” ìˆ˜: {config.HOLDER_LIMIT}ê°œ")
    logger.info(f"ì„±ëŠ¥ ìµœì í™” ì„¤ì •:")
    logger.info(f"  - ìš”ì²­ ì§€ì—°: {config.REQUEST_DELAY}ì´ˆ (ì§€í„° ì ìš©)")
    logger.info(f"  - íƒ€ì„ì•„ì›ƒ: {config.REQUEST_TIMEOUT}ì´ˆ")
    logger.info(f"  - Receipt ë³´ì •: {config.USE_RECEIPT_GAS} (ì‹¤íŒ¨ Top-N {config.ENRICH_RECEIPT_TOPN})")

    try:
        analyzer = EtherscanAnalyzer(api_key=api_key, config=config, logger=logger)
        analyzer.token_address = token_address  # ë³´ê³ ì„œ í‘œê¸°ìš©
    except ValueError as e:
        logger.error(f"ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        print(f"\nâŒ ì—ëŸ¬: {e}")
        print('PowerShellì—ì„œ ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì •í•˜ì„¸ìš”:')
        print('$env:ETHERSCAN_API_KEY = "ì‹¤ì œ_API_í‚¤"')
        print('$env:TOKEN_ADDRESS = "í† í°_ì£¼ì†Œ"')
        print('$env:TOKEN_NAME = "í† í°ì´ë¦„"')
        print('$env:HOLDER_LIMIT = "í™€ë”ìˆ˜"')
        sys.exit(1)

    print(f"ğŸ” {token_name} í† í°ì˜ ìƒìœ„ {config.HOLDER_LIMIT}ê°œ í™€ë” ìˆ˜ì§‘ ì¤‘...")
    holders = analyzer.get_holders(token_address, limit=config.HOLDER_LIMIT)
    if not holders:
        logger.error("í™€ë” ëª©ë¡ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        print("\nâŒ í™€ë” ëª©ë¡ ìˆ˜ì§‘ ì‹¤íŒ¨. ë‹¤ìŒ ì‚¬í•­ì„ í™•ì¸í•˜ì„¸ìš”:")
        print("1. ETHERSCAN_API_KEYê°€ ì˜¬ë°”ë¥¸ì§€")
        print("2. TOKEN_ADDRESSê°€ ìœ íš¨í•œì§€")
        print("3. API ì¿¼í„°ê°€ ë‚¨ì•„ìˆëŠ”ì§€")
        sys.exit(1)

    logger.info(f"Successfully fetched {len(holders)} {token_name} holders")
    print(f"âœ… {len(holders)}ê°œ í™€ë” ìˆ˜ì§‘ ì™„ë£Œ")

    print(f"\nğŸ” {token_name} í† í° {len(holders)}ê°œ í™€ë” ë¶„ì„ ì‹œì‘...")
    print(f"ì˜ˆìƒ ì†Œìš” ì‹œê°„(ëŒ€ëµ): {len(holders) * config.REQUEST_DELAY / 60:.1f}ë¶„")
    print("ë¶„ì„ ì§„í–‰ ì¤‘... (50ê°œ ë‹¨ìœ„ë¡œ ì§„í–‰ë¥  í‘œì‹œ)")

    start_time = time.time()
    tx_df, summary_df, cluster_df = analyzer.analyze_multiple_holders(holders)
    elapsed_minutes = (time.time() - start_time) / 60.0
    print(f"âœ… ë¶„ì„ ì™„ë£Œ! ì‹¤ì œ ì†Œìš” ì‹œê°„: {elapsed_minutes:.1f}ë¶„")

    print("\n" + "="*60)
    print(analyzer.generate_report(tx_df, summary_df, cluster_df, token_name))

    if not summary_df.empty:
        print(f"\n=== {token_name} ìƒìœ„ ìœ„í—˜ í™€ë” (ì‹¤íŒ¨ìœ¨ ê¸°ì¤€) ===")
        risky = summary_df[summary_df["fail_rate"] >= 0.3].sort_values(
            ["fail_rate", "total_gas_waste_eth"], ascending=False
        )
        if not risky.empty:
            display_cols = ["holder", "tx_to_router", "fails", "fail_rate", "sell_fail_rate", "total_gas_waste_eth", "clusters_2h_count"]
            print(risky[display_cols].head(20).to_string(index=False))
        else:
            print("ì‹¤íŒ¨ìœ¨ 30% ì´ìƒì¸ í™€ë”ê°€ ì—†ìŠµë‹ˆë‹¤.")

    if not tx_df.empty:
        print(f"\n=== {token_name} ìµœê·¼ ì‹¤íŒ¨ ê±°ë˜ ë¶„ì„ ===")
        recent_fails = tx_df[tx_df["status"] == "fail"].sort_values("time_utc", ascending=False).head(15)
        if not recent_fails.empty:
            cols = ["holder", "time_utc", "method_guess", "gas_fee_eth", "is_sell_path"]
            print(recent_fails[cols].to_string(index=False))

    saved_files = analyzer.save_results(tx_df, summary_df, cluster_df, token_name)
    if saved_files:
        print(f"\nğŸ“ ì €ì¥ëœ íŒŒì¼ë“¤:")
        for k, v in saved_files.items():
            print(f"  - {k}: {v}")

    print(f"\nâœ… {token_name} í† í° ë¶„ì„ ì™„ë£Œ")
    print(f"ì´ ë¶„ì„ í™€ë” ìˆ˜: {len(holders)}ê°œ")
    print(f"ì´ íŠ¸ëœì­ì…˜ ìˆ˜: {len(tx_df):,}ê°œ" if not tx_df.empty else "ì´ íŠ¸ëœì­ì…˜ ìˆ˜: 0ê°œ")
    print(f"ì‹¤ì œ ë¶„ì„ ì‹œê°„: {elapsed_minutes:.1f}ë¶„")

if __name__ == "__main__":
    main()
